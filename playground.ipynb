{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.selenium_manager import SeleniumManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "from googletrans import Translator\n",
    "from google_trans_new import google_translator  \n",
    "from deep_translator import GoogleTranslator\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "import pyspark.pandas as ps\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(s,source='auto'):\n",
    "        return GoogleTranslator(source=source, target='en').translate(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbselect 2023\n",
      "Clicked 2023 for dbselect\n",
      "district_id मुंबई उपनगर\n",
      "Clicked मुंबई उपनगर for district_id\n",
      "taluka_id अंधेरी\n",
      "Clicked अंधेरी for taluka_id\n",
      "village_id बांद्रा\n",
      "Clicked बांद्रा for village_id\n",
      "Entered Captcha TMSHRU\n",
      "Set to All Records\n",
      "So Far, So Good\n"
     ]
    }
   ],
   "source": [
    "input_params_dropdown_by_id = {}\n",
    "input_params_dropdown_by_id['dbselect'] = '2023' #Select Year\n",
    "input_params_dropdown_by_id['district_id'] = 'मुंबई उपनगर' #District\n",
    "input_params_dropdown_by_id['taluka_id'] = 'अंधेरी' #Taluka\n",
    "input_params_dropdown_by_id['village_id'] = 'बांद्रा' #Village\n",
    "\n",
    "input_params_text_by_id = {}\n",
    "input_params_text_by_id['free_text'] = '2023' \n",
    "\n",
    "url = 'https://pay2igr.igrmaharashtra.gov.in/eDisplay/Propertydetails/index' \n",
    "driver = webdriver.Chrome() \n",
    "driver.implicitly_wait(10)\n",
    "driver.get(url)\n",
    "\n",
    "for key, value in input_params_dropdown_by_id.items():\n",
    "\tprint(key,value)\n",
    "\tdropdown = driver.find_element(by=By.ID,value=key)\n",
    "\tdropdown_select = Select(dropdown)\n",
    "\tfor option in dropdown_select.options:\n",
    "\t\tif option.text == value:\n",
    "\t\t\toption.click()\n",
    "\t\t\tprint(f'Clicked {option.text} for {key}')\n",
    "\t\t\tbreak\n",
    "\tdriver.implicitly_wait(2)\n",
    "\n",
    "#Input reg year\n",
    "driver.find_element(by=By.ID, value='free_text').send_keys(input_params_text_by_id['free_text'])\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "#Input Captcha\n",
    "captcha = input()\n",
    "driver.find_element(by=By.ID, value='cpatchaTextBox').send_keys(captcha)\n",
    "print(f'Entered Captcha {captcha}')\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "#Submit to get result set\n",
    "driver.find_element(by=By.ID,value='submit').click()\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "\n",
    "#Click on 50 Pages\n",
    "dropdown = driver.find_element(by=By.NAME, value='tableparty_length')\n",
    "dropdown_select = Select(dropdown)\n",
    "for option in dropdown_select.options:\n",
    "\tif option.text == 'All':\n",
    "\t\toption.click()\n",
    "\t\tprint('Set to All Records')\n",
    "\t\tbreak\n",
    "\n",
    "print('So Far, So Good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Row appended\n",
      "1 Row appended\n",
      "2 Row appended\n",
      "3 Row appended\n",
      "4 Row appended\n",
      "5 Row appended\n",
      "6 Row appended\n",
      "7 Row appended\n",
      "8 Row appended\n",
      "9 Row appended\n",
      "10 Row appended\n",
      "11 Row appended\n",
      "12 Row appended\n",
      "13 Row appended\n",
      "14 Row appended\n",
      "15 Row appended\n",
      "16 Row appended\n",
      "17 Row appended\n",
      "18 Row appended\n",
      "19 Row appended\n",
      "20 Row appended\n",
      "21 Row appended\n",
      "22 Row appended\n",
      "23 Row appended\n",
      "24 Row appended\n",
      "25 Row appended\n",
      "26 Row appended\n",
      "27 Row appended\n",
      "28 Row appended\n",
      "29 Row appended\n",
      "30 Row appended\n",
      "31 Row appended\n",
      "32 Row appended\n",
      "33 Row appended\n",
      "34 Row appended\n",
      "35 Row appended\n",
      "36 Row appended\n",
      "37 Row appended\n",
      "38 Row appended\n",
      "39 Row appended\n",
      "40 Row appended\n",
      "41 Row appended\n",
      "42 Row appended\n",
      "43 Row appended\n",
      "44 Row appended\n",
      "45 Row appended\n",
      "46 Row appended\n",
      "47 Row appended\n",
      "48 Row appended\n",
      "49 Row appended\n",
      "50 Row appended\n",
      "51 Row appended\n",
      "52 Row appended\n",
      "53 Row appended\n",
      "54 Row appended\n",
      "55 Row appended\n",
      "56 Row appended\n",
      "57 Row appended\n",
      "58 Row appended\n",
      "59 Row appended\n",
      "60 Row appended\n",
      "61 Row appended\n",
      "62 Row appended\n",
      "63 Row appended\n",
      "64 Row appended\n",
      "65 Row appended\n",
      "66 Row appended\n",
      "67 Row appended\n",
      "68 Row appended\n",
      "69 Row appended\n",
      "70 Row appended\n",
      "71 Row appended\n",
      "72 Row appended\n",
      "73 Row appended\n",
      "74 Row appended\n",
      "75 Row appended\n",
      "76 Row appended\n",
      "77 Row appended\n",
      "78 Row appended\n",
      "79 Row appended\n",
      "80 Row appended\n",
      "81 Row appended\n",
      "82 Row appended\n",
      "83 Row appended\n",
      "84 Row appended\n",
      "85 Row appended\n",
      "86 Row appended\n",
      "87 Row appended\n",
      "88 Row appended\n",
      "89 Row appended\n",
      "90 Row appended\n",
      "91 Row appended\n",
      "92 Row appended\n",
      "93 Row appended\n",
      "94 Row appended\n",
      "95 Row appended\n",
      "96 Row appended\n",
      "97 Row appended\n",
      "98 Row appended\n",
      "99 Row appended\n",
      "100 Row appended\n",
      "101 Row appended\n",
      "102 Row appended\n",
      "103 Row appended\n",
      "104 Row appended\n",
      "105 Row appended\n",
      "106 Row appended\n",
      "107 Row appended\n",
      "108 Row appended\n",
      "109 Row appended\n",
      "110 Row appended\n",
      "111 Row appended\n",
      "112 Row appended\n",
      "113 Row appended\n",
      "114 Row appended\n",
      "115 Row appended\n",
      "116 Row appended\n",
      "117 Row appended\n",
      "118 Row appended\n",
      "119 Row appended\n",
      "120 Row appended\n",
      "121 Row appended\n",
      "122 Row appended\n",
      "123 Row appended\n",
      "124 Row appended\n",
      "125 Row appended\n",
      "126 Row appended\n",
      "127 Row appended\n",
      "128 Row appended\n",
      "129 Row appended\n",
      "130 Row appended\n",
      "131 Row appended\n",
      "132 Row appended\n",
      "133 Row appended\n",
      "134 Row appended\n",
      "135 Row appended\n",
      "136 Row appended\n",
      "137 Row appended\n",
      "138 Row appended\n",
      "139 Row appended\n",
      "140 Row appended\n",
      "141 Row appended\n",
      "142 Row appended\n",
      "143 Row appended\n",
      "144 Row appended\n",
      "145 Row appended\n",
      "146 Row appended\n",
      "147 Row appended\n",
      "148 Row appended\n",
      "149 Row appended\n",
      "150 Row appended\n",
      "151 Row appended\n",
      "152 Row appended\n",
      "153 Row appended\n",
      "154 Row appended\n",
      "155 Row appended\n",
      "156 Row appended\n",
      "157 Row appended\n",
      "158 Row appended\n",
      "159 Row appended\n",
      "160 Row appended\n",
      "161 Row appended\n",
      "162 Row appended\n",
      "163 Row appended\n",
      "164 Row appended\n",
      "165 Row appended\n",
      "166 Row appended\n",
      "167 Row appended\n",
      "168 Row appended\n",
      "169 Row appended\n",
      "170 Row appended\n",
      "171 Row appended\n",
      "172 Row appended\n",
      "173 Row appended\n",
      "174 Row appended\n",
      "175 Row appended\n",
      "176 Row appended\n",
      "177 Row appended\n",
      "178 Row appended\n",
      "179 Row appended\n",
      "180 Row appended\n",
      "181 Row appended\n",
      "182 Row appended\n",
      "183 Row appended\n",
      "184 Row appended\n",
      "185 Row appended\n",
      "186 Row appended\n",
      "187 Row appended\n",
      "188 Row appended\n",
      "189 Row appended\n",
      "190 Row appended\n",
      "191 Row appended\n",
      "192 Row appended\n",
      "193 Row appended\n",
      "194 Row appended\n",
      "195 Row appended\n",
      "196 Row appended\n",
      "197 Row appended\n",
      "198 Row appended\n",
      "199 Row appended\n",
      "200 Row appended\n",
      "201 Row appended\n",
      "202 Row appended\n",
      "203 Row appended\n",
      "204 Row appended\n",
      "205 Row appended\n",
      "206 Row appended\n",
      "207 Row appended\n",
      "208 Row appended\n",
      "209 Row appended\n",
      "210 Row appended\n",
      "211 Row appended\n",
      "212 Row appended\n",
      "213 Row appended\n",
      "214 Row appended\n",
      "215 Row appended\n",
      "216 Row appended\n",
      "217 Row appended\n",
      "218 Row appended\n",
      "219 Row appended\n",
      "220 Row appended\n",
      "221 Row appended\n",
      "222 Row appended\n",
      "223 Row appended\n",
      "224 Row appended\n",
      "225 Row appended\n",
      "226 Row appended\n",
      "227 Row appended\n",
      "228 Row appended\n",
      "229 Row appended\n",
      "230 Row appended\n",
      "231 Row appended\n",
      "232 Row appended\n",
      "233 Row appended\n",
      "234 Row appended\n",
      "235 Row appended\n",
      "236 Row appended\n",
      "237 Row appended\n",
      "238 Row appended\n",
      "239 Row appended\n",
      "240 Row appended\n",
      "241 Row appended\n",
      "242 Row appended\n",
      "243 Row appended\n",
      "244 Row appended\n",
      "245 Row appended\n",
      "246 Row appended\n",
      "247 Row appended\n",
      "248 Row appended\n",
      "249 Row appended\n",
      "250 Row appended\n",
      "251 Row appended\n",
      "252 Row appended\n",
      "253 Row appended\n",
      "254 Row appended\n",
      "255 Row appended\n",
      "256 Row appended\n",
      "257 Row appended\n",
      "258 Row appended\n",
      "259 Row appended\n",
      "260 Row appended\n",
      "261 Row appended\n",
      "262 Row appended\n",
      "263 Row appended\n",
      "264 Row appended\n",
      "265 Row appended\n",
      "266 Row appended\n",
      "267 Row appended\n",
      "268 Row appended\n",
      "269 Row appended\n",
      "270 Row appended\n",
      "271 Row appended\n",
      "272 Row appended\n",
      "273 Row appended\n",
      "274 Row appended\n",
      "275 Row appended\n",
      "276 Row appended\n",
      "277 Row appended\n",
      "278 Row appended\n",
      "279 Row appended\n",
      "280 Row appended\n",
      "281 Row appended\n",
      "282 Row appended\n",
      "283 Row appended\n",
      "284 Row appended\n",
      "285 Row appended\n",
      "286 Row appended\n",
      "287 Row appended\n",
      "288 Row appended\n",
      "289 Row appended\n",
      "290 Row appended\n",
      "291 Row appended\n",
      "292 Row appended\n",
      "293 Row appended\n",
      "294 Row appended\n",
      "295 Row appended\n",
      "296 Row appended\n",
      "297 Row appended\n",
      "298 Row appended\n",
      "299 Row appended\n",
      "300 Row appended\n",
      "301 Row appended\n",
      "302 Row appended\n",
      "303 Row appended\n",
      "304 Row appended\n",
      "305 Row appended\n",
      "306 Row appended\n",
      "307 Row appended\n",
      "308 Row appended\n",
      "309 Row appended\n",
      "310 Row appended\n",
      "311 Row appended\n",
      "312 Row appended\n",
      "313 Row appended\n",
      "314 Row appended\n",
      "315 Row appended\n",
      "316 Row appended\n",
      "317 Row appended\n",
      "318 Row appended\n",
      "319 Row appended\n",
      "320 Row appended\n",
      "321 Row appended\n",
      "322 Row appended\n",
      "323 Row appended\n",
      "324 Row appended\n",
      "325 Row appended\n",
      "326 Row appended\n",
      "327 Row appended\n",
      "328 Row appended\n",
      "329 Row appended\n",
      "330 Row appended\n",
      "331 Row appended\n",
      "332 Row appended\n",
      "333 Row appended\n",
      "334 Row appended\n",
      "335 Row appended\n",
      "336 Row appended\n",
      "337 Row appended\n",
      "338 Row appended\n",
      "339 Row appended\n",
      "340 Row appended\n",
      "341 Row appended\n",
      "342 Row appended\n",
      "343 Row appended\n",
      "344 Row appended\n",
      "345 Row appended\n",
      "346 Row appended\n",
      "347 Row appended\n",
      "348 Row appended\n",
      "349 Row appended\n",
      "350 Row appended\n",
      "351 Row appended\n",
      "352 Row appended\n",
      "353 Row appended\n",
      "354 Row appended\n",
      "355 Row appended\n",
      "356 Row appended\n",
      "357 Row appended\n",
      "358 Row appended\n",
      "359 Row appended\n",
      "360 Row appended\n",
      "361 Row appended\n",
      "362 Row appended\n",
      "363 Row appended\n",
      "364 Row appended\n",
      "365 Row appended\n",
      "366 Row appended\n",
      "367 Row appended\n",
      "368 Row appended\n",
      "369 Row appended\n",
      "370 Row appended\n",
      "371 Row appended\n",
      "372 Row appended\n",
      "373 Row appended\n",
      "374 Row appended\n",
      "375 Row appended\n",
      "376 Row appended\n",
      "377 Row appended\n",
      "378 Row appended\n",
      "379 Row appended\n",
      "380 Row appended\n",
      "381 Row appended\n",
      "382 Row appended\n",
      "383 Row appended\n",
      "384 Row appended\n",
      "385 Row appended\n",
      "386 Row appended\n",
      "387 Row appended\n",
      "388 Row appended\n",
      "389 Row appended\n",
      "390 Row appended\n",
      "391 Row appended\n",
      "392 Row appended\n",
      "393 Row appended\n",
      "394 Row appended\n",
      "395 Row appended\n",
      "396 Row appended\n",
      "397 Row appended\n",
      "398 Row appended\n",
      "399 Row appended\n",
      "400 Row appended\n",
      "401 Row appended\n",
      "402 Row appended\n",
      "403 Row appended\n",
      "404 Row appended\n",
      "405 Row appended\n",
      "406 Row appended\n",
      "407 Row appended\n",
      "408 Row appended\n",
      "409 Row appended\n",
      "410 Row appended\n",
      "411 Row appended\n",
      "412 Row appended\n",
      "413 Row appended\n",
      "414 Row appended\n",
      "415 Row appended\n",
      "416 Row appended\n",
      "417 Row appended\n",
      "418 Row appended\n",
      "419 Row appended\n",
      "420 Row appended\n",
      "421 Row appended\n",
      "422 Row appended\n",
      "423 Row appended\n",
      "424 Row appended\n",
      "425 Row appended\n",
      "426 Row appended\n",
      "427 Row appended\n",
      "428 Row appended\n",
      "429 Row appended\n",
      "430 Row appended\n",
      "431 Row appended\n",
      "432 Row appended\n",
      "433 Row appended\n",
      "434 Row appended\n",
      "435 Row appended\n",
      "436 Row appended\n",
      "437 Row appended\n",
      "438 Row appended\n",
      "439 Row appended\n",
      "440 Row appended\n",
      "441 Row appended\n",
      "442 Row appended\n",
      "443 Row appended\n",
      "444 Row appended\n",
      "445 Row appended\n",
      "446 Row appended\n",
      "447 Row appended\n",
      "448 Row appended\n",
      "449 Row appended\n",
      "450 Row appended\n",
      "451 Row appended\n",
      "452 Row appended\n",
      "453 Row appended\n",
      "454 Row appended\n",
      "455 Row appended\n",
      "456 Row appended\n",
      "457 Row appended\n",
      "458 Row appended\n",
      "459 Row appended\n",
      "460 Row appended\n",
      "461 Row appended\n",
      "462 Row appended\n",
      "463 Row appended\n",
      "464 Row appended\n",
      "465 Row appended\n",
      "466 Row appended\n",
      "467 Row appended\n",
      "468 Row appended\n",
      "469 Row appended\n",
      "470 Row appended\n",
      "471 Row appended\n",
      "472 Row appended\n",
      "473 Row appended\n",
      "474 Row appended\n",
      "475 Row appended\n",
      "476 Row appended\n",
      "477 Row appended\n",
      "478 Row appended\n",
      "479 Row appended\n",
      "480 Row appended\n",
      "481 Row appended\n",
      "482 Row appended\n",
      "483 Row appended\n",
      "484 Row appended\n",
      "485 Row appended\n",
      "486 Row appended\n",
      "487 Row appended\n",
      "488 Row appended\n",
      "489 Row appended\n",
      "490 Row appended\n",
      "491 Row appended\n",
      "492 Row appended\n",
      "493 Row appended\n",
      "494 Row appended\n",
      "495 Row appended\n",
      "496 Row appended\n",
      "497 Row appended\n",
      "498 Row appended\n",
      "499 Row appended\n",
      "500 Row appended\n",
      "501 Row appended\n",
      "502 Row appended\n",
      "503 Row appended\n",
      "504 Row appended\n",
      "505 Row appended\n",
      "506 Row appended\n",
      "507 Row appended\n",
      "508 Row appended\n",
      "509 Row appended\n",
      "510 Row appended\n",
      "511 Row appended\n",
      "512 Row appended\n",
      "513 Row appended\n",
      "514 Row appended\n",
      "515 Row appended\n",
      "516 Row appended\n",
      "517 Row appended\n",
      "518 Row appended\n",
      "519 Row appended\n",
      "520 Row appended\n",
      "521 Row appended\n",
      "522 Row appended\n",
      "523 Row appended\n",
      "524 Row appended\n",
      "525 Row appended\n",
      "526 Row appended\n",
      "527 Row appended\n",
      "528 Row appended\n",
      "529 Row appended\n",
      "530 Row appended\n",
      "531 Row appended\n",
      "532 Row appended\n",
      "533 Row appended\n",
      "534 Row appended\n",
      "535 Row appended\n",
      "536 Row appended\n",
      "537 Row appended\n",
      "538 Row appended\n",
      "539 Row appended\n",
      "540 Row appended\n",
      "541 Row appended\n",
      "542 Row appended\n",
      "543 Row appended\n",
      "544 Row appended\n",
      "545 Row appended\n",
      "546 Row appended\n",
      "547 Row appended\n",
      "548 Row appended\n",
      "549 Row appended\n",
      "550 Row appended\n",
      "551 Row appended\n",
      "552 Row appended\n",
      "553 Row appended\n",
      "554 Row appended\n",
      "555 Row appended\n",
      "556 Row appended\n",
      "557 Row appended\n",
      "558 Row appended\n",
      "559 Row appended\n",
      "560 Row appended\n",
      "561 Row appended\n",
      "562 Row appended\n",
      "563 Row appended\n",
      "564 Row appended\n",
      "565 Row appended\n",
      "566 Row appended\n",
      "567 Row appended\n",
      "568 Row appended\n",
      "569 Row appended\n",
      "570 Row appended\n",
      "571 Row appended\n",
      "572 Row appended\n",
      "573 Row appended\n",
      "574 Row appended\n",
      "575 Row appended\n",
      "576 Row appended\n",
      "577 Row appended\n",
      "578 Row appended\n",
      "579 Row appended\n",
      "580 Row appended\n",
      "581 Row appended\n",
      "582 Row appended\n",
      "583 Row appended\n",
      "584 Row appended\n",
      "585 Row appended\n",
      "586 Row appended\n",
      "587 Row appended\n",
      "588 Row appended\n",
      "589 Row appended\n",
      "590 Row appended\n",
      "591 Row appended\n",
      "592 Row appended\n",
      "593 Row appended\n",
      "594 Row appended\n",
      "595 Row appended\n",
      "596 Row appended\n",
      "597 Row appended\n",
      "598 Row appended\n",
      "599 Row appended\n",
      "600 Row appended\n",
      "601 Row appended\n",
      "602 Row appended\n",
      "603 Row appended\n",
      "604 Row appended\n",
      "605 Row appended\n",
      "606 Row appended\n",
      "607 Row appended\n",
      "608 Row appended\n",
      "609 Row appended\n",
      "610 Row appended\n",
      "611 Row appended\n",
      "612 Row appended\n",
      "613 Row appended\n",
      "614 Row appended\n",
      "615 Row appended\n",
      "616 Row appended\n",
      "617 Row appended\n",
      "618 Row appended\n",
      "619 Row appended\n",
      "620 Row appended\n",
      "621 Row appended\n",
      "622 Row appended\n",
      "623 Row appended\n",
      "624 Row appended\n",
      "625 Row appended\n",
      "626 Row appended\n",
      "627 Row appended\n",
      "628 Row appended\n",
      "629 Row appended\n",
      "630 Row appended\n",
      "631 Row appended\n",
      "632 Row appended\n",
      "633 Row appended\n",
      "634 Row appended\n",
      "635 Row appended\n",
      "636 Row appended\n",
      "637 Row appended\n",
      "638 Row appended\n",
      "639 Row appended\n",
      "640 Row appended\n",
      "641 Row appended\n",
      "642 Row appended\n",
      "643 Row appended\n",
      "644 Row appended\n",
      "645 Row appended\n",
      "646 Row appended\n",
      "647 Row appended\n",
      "648 Row appended\n",
      "649 Row appended\n",
      "650 Row appended\n",
      "651 Row appended\n",
      "652 Row appended\n",
      "653 Row appended\n",
      "654 Row appended\n",
      "655 Row appended\n",
      "656 Row appended\n",
      "657 Row appended\n",
      "658 Row appended\n",
      "659 Row appended\n",
      "660 Row appended\n",
      "661 Row appended\n",
      "662 Row appended\n",
      "663 Row appended\n",
      "664 Row appended\n",
      "665 Row appended\n",
      "666 Row appended\n",
      "667 Row appended\n",
      "668 Row appended\n",
      "669 Row appended\n",
      "670 Row appended\n",
      "671 Row appended\n",
      "672 Row appended\n",
      "673 Row appended\n",
      "674 Row appended\n",
      "675 Row appended\n",
      "676 Row appended\n",
      "677 Row appended\n",
      "678 Row appended\n",
      "679 Row appended\n",
      "680 Row appended\n",
      "681 Row appended\n",
      "682 Row appended\n",
      "683 Row appended\n",
      "684 Row appended\n",
      "685 Row appended\n",
      "686 Row appended\n",
      "687 Row appended\n",
      "688 Row appended\n",
      "689 Row appended\n",
      "690 Row appended\n",
      "691 Row appended\n",
      "692 Row appended\n",
      "693 Row appended\n",
      "694 Row appended\n",
      "695 Row appended\n",
      "696 Row appended\n",
      "697 Row appended\n",
      "698 Row appended\n",
      "699 Row appended\n",
      "700 Row appended\n",
      "701 Row appended\n",
      "702 Row appended\n",
      "703 Row appended\n",
      "704 Row appended\n",
      "705 Row appended\n",
      "706 Row appended\n",
      "707 Row appended\n",
      "708 Row appended\n",
      "709 Row appended\n",
      "710 Row appended\n",
      "711 Row appended\n",
      "712 Row appended\n",
      "713 Row appended\n",
      "714 Row appended\n",
      "715 Row appended\n",
      "716 Row appended\n",
      "717 Row appended\n",
      "718 Row appended\n",
      "719 Row appended\n",
      "720 Row appended\n",
      "721 Row appended\n",
      "722 Row appended\n",
      "723 Row appended\n",
      "724 Row appended\n",
      "725 Row appended\n",
      "726 Row appended\n",
      "727 Row appended\n",
      "728 Row appended\n",
      "729 Row appended\n",
      "730 Row appended\n",
      "731 Row appended\n",
      "732 Row appended\n",
      "733 Row appended\n",
      "734 Row appended\n",
      "735 Row appended\n",
      "736 Row appended\n",
      "737 Row appended\n",
      "738 Row appended\n",
      "739 Row appended\n",
      "740 Row appended\n",
      "741 Row appended\n",
      "742 Row appended\n",
      "743 Row appended\n",
      "744 Row appended\n",
      "745 Row appended\n",
      "746 Row appended\n",
      "747 Row appended\n",
      "748 Row appended\n",
      "749 Row appended\n",
      "750 Row appended\n",
      "751 Row appended\n",
      "752 Row appended\n",
      "753 Row appended\n",
      "754 Row appended\n",
      "755 Row appended\n",
      "756 Row appended\n",
      "757 Row appended\n",
      "758 Row appended\n",
      "759 Row appended\n",
      "760 Row appended\n",
      "761 Row appended\n",
      "762 Row appended\n",
      "763 Row appended\n",
      "764 Row appended\n",
      "765 Row appended\n",
      "766 Row appended\n",
      "767 Row appended\n",
      "768 Row appended\n",
      "769 Row appended\n",
      "770 Row appended\n",
      "771 Row appended\n",
      "772 Row appended\n",
      "773 Row appended\n",
      "774 Row appended\n",
      "775 Row appended\n",
      "776 Row appended\n",
      "777 Row appended\n",
      "778 Row appended\n",
      "779 Row appended\n",
      "780 Row appended\n",
      "781 Row appended\n",
      "782 Row appended\n",
      "783 Row appended\n",
      "784 Row appended\n",
      "785 Row appended\n",
      "786 Row appended\n",
      "787 Row appended\n",
      "788 Row appended\n",
      "789 Row appended\n",
      "790 Row appended\n",
      "791 Row appended\n",
      "792 Row appended\n",
      "793 Row appended\n",
      "794 Row appended\n",
      "795 Row appended\n",
      "796 Row appended\n",
      "797 Row appended\n",
      "798 Row appended\n",
      "799 Row appended\n",
      "800 Row appended\n",
      "801 Row appended\n",
      "802 Row appended\n",
      "803 Row appended\n",
      "804 Row appended\n",
      "805 Row appended\n",
      "806 Row appended\n",
      "807 Row appended\n",
      "808 Row appended\n",
      "809 Row appended\n",
      "810 Row appended\n",
      "811 Row appended\n",
      "812 Row appended\n",
      "813 Row appended\n",
      "814 Row appended\n",
      "815 Row appended\n",
      "816 Row appended\n",
      "817 Row appended\n",
      "818 Row appended\n",
      "819 Row appended\n",
      "820 Row appended\n",
      "821 Row appended\n",
      "822 Row appended\n",
      "823 Row appended\n",
      "824 Row appended\n",
      "825 Row appended\n",
      "826 Row appended\n",
      "827 Row appended\n",
      "828 Row appended\n",
      "829 Row appended\n",
      "830 Row appended\n",
      "831 Row appended\n",
      "832 Row appended\n",
      "833 Row appended\n",
      "834 Row appended\n",
      "835 Row appended\n",
      "836 Row appended\n",
      "837 Row appended\n",
      "838 Row appended\n",
      "839 Row appended\n",
      "840 Row appended\n",
      "841 Row appended\n",
      "842 Row appended\n",
      "843 Row appended\n",
      "844 Row appended\n",
      "845 Row appended\n",
      "846 Row appended\n",
      "847 Row appended\n",
      "848 Row appended\n",
      "849 Row appended\n",
      "850 Row appended\n",
      "851 Row appended\n",
      "852 Row appended\n",
      "853 Row appended\n",
      "854 Row appended\n",
      "855 Row appended\n",
      "856 Row appended\n",
      "857 Row appended\n",
      "858 Row appended\n",
      "859 Row appended\n",
      "860 Row appended\n",
      "861 Row appended\n",
      "862 Row appended\n",
      "863 Row appended\n",
      "864 Row appended\n",
      "865 Row appended\n",
      "866 Row appended\n",
      "867 Row appended\n",
      "868 Row appended\n",
      "869 Row appended\n",
      "870 Row appended\n",
      "871 Row appended\n",
      "872 Row appended\n",
      "873 Row appended\n",
      "874 Row appended\n",
      "875 Row appended\n",
      "876 Row appended\n",
      "877 Row appended\n",
      "878 Row appended\n",
      "879 Row appended\n",
      "880 Row appended\n",
      "881 Row appended\n",
      "882 Row appended\n",
      "883 Row appended\n",
      "884 Row appended\n",
      "885 Row appended\n",
      "886 Row appended\n",
      "887 Row appended\n",
      "888 Row appended\n",
      "889 Row appended\n",
      "890 Row appended\n",
      "891 Row appended\n",
      "892 Row appended\n",
      "893 Row appended\n",
      "894 Row appended\n",
      "895 Row appended\n",
      "896 Row appended\n",
      "897 Row appended\n",
      "898 Row appended\n",
      "899 Row appended\n",
      "900 Row appended\n",
      "901 Row appended\n",
      "902 Row appended\n",
      "903 Row appended\n",
      "904 Row appended\n",
      "905 Row appended\n",
      "906 Row appended\n",
      "907 Row appended\n",
      "908 Row appended\n",
      "909 Row appended\n",
      "910 Row appended\n",
      "911 Row appended\n",
      "912 Row appended\n",
      "913 Row appended\n",
      "914 Row appended\n",
      "915 Row appended\n",
      "916 Row appended\n",
      "917 Row appended\n",
      "918 Row appended\n",
      "919 Row appended\n",
      "920 Row appended\n",
      "921 Row appended\n",
      "922 Row appended\n",
      "923 Row appended\n",
      "924 Row appended\n",
      "925 Row appended\n",
      "926 Row appended\n",
      "927 Row appended\n",
      "928 Row appended\n",
      "929 Row appended\n",
      "930 Row appended\n",
      "931 Row appended\n",
      "932 Row appended\n",
      "933 Row appended\n",
      "934 Row appended\n",
      "935 Row appended\n",
      "936 Row appended\n",
      "937 Row appended\n",
      "938 Row appended\n",
      "939 Row appended\n",
      "940 Row appended\n",
      "941 Row appended\n",
      "942 Row appended\n",
      "943 Row appended\n",
      "944 Row appended\n",
      "945 Row appended\n",
      "946 Row appended\n",
      "947 Row appended\n",
      "948 Row appended\n",
      "949 Row appended\n",
      "950 Row appended\n",
      "951 Row appended\n",
      "952 Row appended\n",
      "953 Row appended\n",
      "954 Row appended\n",
      "955 Row appended\n",
      "956 Row appended\n",
      "957 Row appended\n",
      "958 Row appended\n",
      "959 Row appended\n",
      "960 Row appended\n",
      "961 Row appended\n",
      "962 Row appended\n",
      "963 Row appended\n",
      "964 Row appended\n",
      "965 Row appended\n",
      "966 Row appended\n",
      "967 Row appended\n",
      "968 Row appended\n",
      "969 Row appended\n",
      "970 Row appended\n",
      "971 Row appended\n",
      "972 Row appended\n",
      "973 Row appended\n",
      "974 Row appended\n",
      "975 Row appended\n",
      "976 Row appended\n",
      "977 Row appended\n",
      "978 Row appended\n",
      "979 Row appended\n",
      "980 Row appended\n",
      "981 Row appended\n",
      "982 Row appended\n",
      "983 Row appended\n",
      "984 Row appended\n",
      "985 Row appended\n",
      "986 Row appended\n",
      "987 Row appended\n",
      "988 Row appended\n",
      "989 Row appended\n",
      "990 Row appended\n",
      "991 Row appended\n",
      "992 Row appended\n",
      "993 Row appended\n",
      "994 Row appended\n",
      "995 Row appended\n",
      "996 Row appended\n",
      "997 Row appended\n",
      "998 Row appended\n",
      "999 Row appended\n",
      "1000 Row appended\n",
      "1001 Row appended\n",
      "1002 Row appended\n",
      "1003 Row appended\n",
      "1004 Row appended\n",
      "1005 Row appended\n",
      "1006 Row appended\n",
      "1007 Row appended\n",
      "1008 Row appended\n",
      "1009 Row appended\n",
      "1010 Row appended\n",
      "1011 Row appended\n",
      "1012 Row appended\n",
      "1013 Row appended\n",
      "1014 Row appended\n",
      "1015 Row appended\n",
      "1016 Row appended\n",
      "1017 Row appended\n",
      "1018 Row appended\n",
      "1019 Row appended\n",
      "1020 Row appended\n",
      "1021 Row appended\n",
      "1022 Row appended\n",
      "1023 Row appended\n",
      "1024 Row appended\n",
      "1025 Row appended\n",
      "1026 Row appended\n",
      "1027 Row appended\n",
      "1028 Row appended\n",
      "1029 Row appended\n",
      "1030 Row appended\n",
      "1031 Row appended\n",
      "1032 Row appended\n",
      "1033 Row appended\n",
      "1034 Row appended\n",
      "1035 Row appended\n",
      "1036 Row appended\n",
      "1037 Row appended\n",
      "1038 Row appended\n",
      "1039 Row appended\n",
      "1040 Row appended\n",
      "1041 Row appended\n",
      "1042 Row appended\n",
      "1043 Row appended\n",
      "1044 Row appended\n",
      "1045 Row appended\n",
      "1046 Row appended\n",
      "1047 Row appended\n",
      "1048 Row appended\n",
      "1049 Row appended\n",
      "1050 Row appended\n",
      "1051 Row appended\n",
      "1052 Row appended\n",
      "1053 Row appended\n",
      "1054 Row appended\n",
      "1055 Row appended\n",
      "1056 Row appended\n",
      "1057 Row appended\n",
      "1058 Row appended\n",
      "1059 Row appended\n",
      "1060 Row appended\n",
      "1061 Row appended\n",
      "1062 Row appended\n",
      "1063 Row appended\n",
      "1064 Row appended\n",
      "1065 Row appended\n",
      "1066 Row appended\n",
      "1067 Row appended\n",
      "1068 Row appended\n",
      "1069 Row appended\n",
      "1070 Row appended\n",
      "1071 Row appended\n",
      "1072 Row appended\n",
      "1073 Row appended\n",
      "1074 Row appended\n",
      "1075 Row appended\n",
      "1076 Row appended\n",
      "1077 Row appended\n",
      "1078 Row appended\n",
      "1079 Row appended\n",
      "1080 Row appended\n",
      "1081 Row appended\n",
      "1082 Row appended\n",
      "1083 Row appended\n",
      "1084 Row appended\n",
      "1085 Row appended\n",
      "1086 Row appended\n",
      "1087 Row appended\n",
      "1088 Row appended\n",
      "1089 Row appended\n",
      "1090 Row appended\n",
      "1091 Row appended\n",
      "1092 Row appended\n",
      "1093 Row appended\n",
      "1094 Row appended\n",
      "1095 Row appended\n",
      "1096 Row appended\n",
      "1097 Row appended\n",
      "1098 Row appended\n",
      "1099 Row appended\n",
      "1100 Row appended\n",
      "1101 Row appended\n",
      "1102 Row appended\n",
      "1103 Row appended\n",
      "1104 Row appended\n",
      "1105 Row appended\n",
      "1106 Row appended\n",
      "1107 Row appended\n",
      "1108 Row appended\n",
      "1109 Row appended\n",
      "1110 Row appended\n",
      "1111 Row appended\n",
      "1112 Row appended\n",
      "1113 Row appended\n",
      "1114 Row appended\n",
      "1115 Row appended\n",
      "1116 Row appended\n",
      "1117 Row appended\n",
      "1118 Row appended\n",
      "1119 Row appended\n",
      "1120 Row appended\n",
      "1121 Row appended\n",
      "1122 Row appended\n",
      "1123 Row appended\n",
      "1124 Row appended\n",
      "1125 Row appended\n",
      "1126 Row appended\n",
      "1127 Row appended\n",
      "1128 Row appended\n",
      "1129 Row appended\n",
      "1130 Row appended\n",
      "1131 Row appended\n",
      "1132 Row appended\n",
      "1133 Row appended\n",
      "1134 Row appended\n",
      "1135 Row appended\n",
      "1136 Row appended\n",
      "1137 Row appended\n",
      "1138 Row appended\n",
      "1139 Row appended\n",
      "1140 Row appended\n",
      "1141 Row appended\n",
      "1142 Row appended\n",
      "1143 Row appended\n",
      "1144 Row appended\n",
      "1145 Row appended\n",
      "1146 Row appended\n",
      "1147 Row appended\n",
      "1148 Row appended\n",
      "1149 Row appended\n",
      "1150 Row appended\n",
      "1151 Row appended\n",
      "1152 Row appended\n",
      "1153 Row appended\n",
      "1154 Row appended\n",
      "1155 Row appended\n",
      "1156 Row appended\n",
      "1157 Row appended\n",
      "1158 Row appended\n",
      "1159 Row appended\n",
      "1160 Row appended\n",
      "1161 Row appended\n",
      "1162 Row appended\n",
      "1163 Row appended\n",
      "1164 Row appended\n",
      "1165 Row appended\n",
      "1166 Row appended\n",
      "1167 Row appended\n",
      "1168 Row appended\n",
      "1169 Row appended\n",
      "1170 Row appended\n",
      "1171 Row appended\n",
      "1172 Row appended\n",
      "1173 Row appended\n",
      "1174 Row appended\n",
      "1175 Row appended\n",
      "1176 Row appended\n",
      "1177 Row appended\n",
      "1178 Row appended\n",
      "1179 Row appended\n",
      "1180 Row appended\n",
      "1181 Row appended\n",
      "1182 Row appended\n",
      "1183 Row appended\n",
      "1184 Row appended\n",
      "1185 Row appended\n",
      "1186 Row appended\n",
      "1187 Row appended\n",
      "1188 Row appended\n",
      "1189 Row appended\n",
      "1190 Row appended\n",
      "1191 Row appended\n",
      "1192 Row appended\n",
      "1193 Row appended\n",
      "1194 Row appended\n",
      "1195 Row appended\n",
      "1196 Row appended\n",
      "1197 Row appended\n",
      "1198 Row appended\n",
      "1199 Row appended\n",
      "1200 Row appended\n",
      "1201 Row appended\n",
      "1202 Row appended\n",
      "1203 Row appended\n",
      "1204 Row appended\n",
      "1205 Row appended\n",
      "1206 Row appended\n",
      "1207 Row appended\n",
      "1208 Row appended\n"
     ]
    }
   ],
   "source": [
    "records_raw = pd.DataFrame(columns=['अनु क्र.','दस्त क्र.','दस्त प्रकार','दू. नि. कार्यालय','वर्ष','लिहून देणार','लिहून घेणार','इतर माहीती','सूची क्र. २'])\n",
    "\n",
    "for index, table in enumerate(driver.find_elements(by=By.ID, value='tbdata')):\n",
    "    data = [item.text if item.text != 'सूची क्र. २' else item.find_element(by=By.TAG_NAME,value='a').get_attribute('href') for item in table.find_elements(by=By.XPATH, value=\".//*[self::td or self::th]\")]\n",
    "    records_raw.loc[len(records_raw)] = data\n",
    "    # print(data)\n",
    "    print(f'{index} Row appended')\n",
    "\n",
    "records_raw\n",
    "records_translate = records_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209 DataFrame has been inserted into the 'record_details_raw' table.\n"
     ]
    }
   ],
   "source": [
    "# PostgreSQL database connection parameters\n",
    "db_user = 'postgres'\n",
    "db_password = 'Sunrise12345'\n",
    "db_host = '127.0.0.1'\n",
    "db_port = '5432'\n",
    "db_name = 'propReturns'\n",
    "\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "table_name = 'record_details_raw'\n",
    "\n",
    "records_raw.to_sql(table_name, engine, if_exists='replace', index=True)\n",
    "\n",
    "print(f\"{len(records_raw)} DataFrame has been inserted into the '{table_name}' table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [0,100]\n",
      "Translating diarrhea type\n",
      "Done\n",
      "Translating Du. Prohibit. Office\n",
      "Done\n",
      "Translating Will write\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "Request exception can happen due to an api connection error. Please check your connection and try again",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_to_translate:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m     temp_df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m records_translated \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([records_translated,temp_df])\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_to_translate:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m     temp_df[col] \u001b[38;5;241m=\u001b[39m temp_df[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m records_translated \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([records_translated,temp_df])\n",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m, in \u001b[0;36mtranslator\u001b[1;34m(s, source)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslator\u001b[39m(s,source):\n\u001b[1;32m----> 2\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGoogleTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\deep_translator\\google.py:74\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_failed(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code):\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError()\n\u001b[0;32m     76\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m element \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_tag, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_query)\n",
      "\u001b[1;31mRequestError\u001b[0m: Request exception can happen due to an api connection error. Please check your connection and try again"
     ]
    }
   ],
   "source": [
    "#Approach Via Pandas\n",
    "records_translate = records_raw.copy()\n",
    "records_translate.columns = [translator(col,source='auto') for col in records_translate.columns]\n",
    "cols_to_translate = [\n",
    "    'diarrhea type',\n",
    "    'Du. Prohibit. Office',\n",
    "    'Will write', \n",
    "    'Will write down', \n",
    "    'Other information'\n",
    "    ]\n",
    "\n",
    "records_translated = pd.DataFrame(columns=records_translate.columns)\n",
    "batch_size = 100\n",
    "start = 0\n",
    "for batch in range(start,len(records_translate),batch_size):\n",
    "    print(f'Batch -> [{batch},{batch+batch_size}]')\n",
    "    temp_df = records_translate.iloc[batch:batch+batch_size,:].copy()\n",
    "    for col in cols_to_translate:\n",
    "        print(f'Translating {col}')\n",
    "        temp_df[col] = temp_df[col].apply(lambda x: translator(x,source='hi'))\n",
    "        print(f'Done')\n",
    "    records_translated = pd.concat([records_translated,temp_df])\n",
    "records_translated.rename(columns={'Will write':'buyer_name','Will write down':'seller_name'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x15d7a6e1420>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach Via Spark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del spark_df \n",
    "spark_df = spark.createDataFrame(records_raw.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for col in records_raw.columns:\n",
    "    new_names.append(translator(col,source='auto'))\n",
    "\n",
    "new_names = [x.replace('.','') for x in new_names]\n",
    "spark_df = spark_df.toDF(*new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Sl no|Diarrhea no|       diarrhea type|Du Prohibit Office|      Year|          Will write|     Will write down|   Other information|           List no 2|\n",
      "+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|      13217|           भाडेपट्टा|सह दु.नि. अंधेरी 7|25/07/2023|1) श्रीमती चंद्रक...|1) महाराष्ट़ हाऊस...|1) इतर माहिती: पि...|https://pay2igr.i...|\n",
      "|    2|       4286|       विकसनकरारनामा|सह दु.नि. अंधेरी 7|10/03/2023|1) अजित सिंह करता...|1) नाविश रियल्टी ...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|    3|       2449|             सेल डीड|सह दु.नि. अंधेरी 7|09/02/2023|1) नूपुर अनिल कळक...|1) हर्ष सतपाल मल्...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|    4|       8691|66-नोटीस ऑफ़ लिस प...|सह दु.नि. अंधेरी 7|22/05/2023|      1) प्रदीप सोनी|                    |1) इतर माहिती: सि...|https://pay2igr.i...|\n",
      "|    5|       3551|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|27/02/2023|1) गुंजन योगीत कप...|1) शामा सुभाष ओबेराय|1) सदनिका नं: 201...|https://pay2igr.i...|\n",
      "|    6|       8043|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|19/05/2023|1) राधिका खंडेलवा...|1) व्हीबीएचडीसी ब...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|    7|       5454|           भाडेपट्टा|सह दु.नि. अंधेरी 7|29/03/2023|1) नियोजन को ऑपरे...|1) मुंबई हाऊसिंग ...|1) सदनिका नं: प्ल...|https://pay2igr.i...|\n",
      "|    8|       7732|    अभिहस्तांतरणपत्र|सह दु.नि. अंधेरी 7|04/05/2023|1) मे एकता सुप्री...|1) युडोरा कॉ ऑप ह...|1) इतर माहिती: इम...|https://pay2igr.i...|\n",
      "|    9|       5457|           भाडेपट्टा|सह दु.नि. अंधेरी 7|29/03/2023|1) साई दत्त प्रसा...|1) मुंबई हाऊसिंग ...|1) सदनिका नं: प्ल...|https://pay2igr.i...|\n",
      "|   10|       2389|       प्रतिज्ञापत्र|सह दु.नि. अंधेरी 7|08/02/2023|1) गांघी नगर गणेश...|                    |1) इतर माहिती: re...|https://pay2igr.i...|\n",
      "|   11|       8896|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|24/05/2023|1) . . गोपाल नारा...|1) . . श्रीधर गोप...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|   12|       8895|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|24/05/2023|1) . . गोपाल नारा...|1) . . नेहा गोपाल...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|   13|       7415|       विकसनकरारनामा|सह दु.नि. अंधेरी 7|02/05/2023|1) सोसायटी मेंबर ...|1) इंस्पायरा बिल्...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   14|        212|             सेल डीड|सह दु.नि. अंधेरी 7|04/01/2023|1) पुष्पा बी रहेज...|1) कलश दिलीप सुरा...|1) सदनिका नं: 501...|https://pay2igr.i...|\n",
      "|   15|       7019|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|21/04/2023|1) मिशेल देसाई पू...|1) किऑर्बिट रिअल्...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   16|       5868|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|12/04/2023|1) बांद्रा विनय क...|1) डायनास्टी इन्फ...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   17|       5450|              लीजडीड|सह दु.नि. अंधेरी 7|29/03/2023|1) दि एम आय जी को...|1) (मालक ) महाराष...|1) इतर माहिती: टी...|https://pay2igr.i...|\n",
      "|   18|       3753|             सेल डीड|सह दु.नि. अंधेरी 7|01/03/2023|1) राहुल विनायक ड...|1) रमेशकुमार मानक...|1) सदनिका नं: 801...|https://pay2igr.i...|\n",
      "|   19|       8517|66-नोटीस ऑफ़ लिस प...|सह दु.नि. अंधेरी 7|18/05/2023|1) व्हिज एंटरप्रा...|                    |1) सदनिका नं: 602...|https://pay2igr.i...|\n",
      "|   20|      12363|        59-हस्तांतरण|सह दु.नि. अंधेरी 7|11/07/2023|1) मयत ताराबेन शा...|1) मुकेश शांतीलाल...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sl no: string (nullable = true)\n",
      " |-- Diarrhea no: string (nullable = true)\n",
      " |-- diarrhea type: string (nullable = true)\n",
      " |-- Du Prohibit Office: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Will write: string (nullable = true)\n",
      " |-- Will write down: string (nullable = true)\n",
      " |-- Other information: string (nullable = true)\n",
      " |-- List no 2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "def translator_v2(s,source='auto'):\n",
    "        translator = Translator()\n",
    "        return translator.translate(s, src='hi',dest='en').text \n",
    "\n",
    "translate_udf = udf(translator_v2,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_udf_og = udf(translator,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Sl no|Diarrhea no|       diarrhea type|Du Prohibit Office|      Year|          Will write|     Will write down|   Other information|           List no 2|\n",
      "+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|      13217|           भाडेपट्टा|सह दु.नि. अंधेरी 7|25/07/2023|1) श्रीमती चंद्रक...|1) Hanmant Dhanwa...|1) इतर माहिती: पि...|https://pay2igr.i...|\n",
      "|    2|       4286|       विकसनकरारनामा|सह दु.नि. अंधेरी 7|10/03/2023|1) अजित सिंह करता...|1) Paresh Ranchho...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|    3|       2449|             सेल डीड|सह दु.नि. अंधेरी 7|09/02/2023|1) नूपुर अनिल कळक...|1) Harsh Satpal M...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|    4|       8691|66-नोटीस ऑफ़ लिस प...|सह दु.नि. अंधेरी 7|22/05/2023|      1) प्रदीप सोनी|                    |1) इतर माहिती: सि...|https://pay2igr.i...|\n",
      "|    5|       3551|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|27/02/2023|1) गुंजन योगीत कप...|1) Shama Subhash ...|1) सदनिका नं: 201...|https://pay2igr.i...|\n",
      "|    6|       8043|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|19/05/2023|1) राधिका खंडेलवा...|1) Suraj Kumar Do...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|    7|       5454|           भाडेपट्टा|सह दु.नि. अंधेरी 7|29/03/2023|1) नियोजन को ऑपरे...|1) Deputy Enginee...|1) सदनिका नं: प्ल...|https://pay2igr.i...|\n",
      "|    8|       7732|    अभिहस्तांतरणपत्र|सह दु.नि. अंधेरी 7|04/05/2023|1) मे एकता सुप्री...|1) Treasurer Ramy...|1) इतर माहिती: इम...|https://pay2igr.i...|\n",
      "|    9|       5457|           भाडेपट्टा|सह दु.नि. अंधेरी 7|29/03/2023|1) साई दत्त प्रसा...|1) Ashok Kajane, ...|1) सदनिका नं: प्ल...|https://pay2igr.i...|\n",
      "|   10|       2389|       प्रतिज्ञापत्र|सह दु.नि. अंधेरी 7|08/02/2023|1) गांघी नगर गणेश...|                    |1) इतर माहिती: re...|https://pay2igr.i...|\n",
      "|   11|       8896|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|24/05/2023|1) . . गोपाल नारा...|1) . , Sridhar Go...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|   12|       8895|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|24/05/2023|1) . . गोपाल नारा...|1) . , Neha Gopal...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|   13|       7415|       विकसनकरारनामा|सह दु.नि. अंधेरी 7|02/05/2023|1) सोसायटी मेंबर ...|1) Authorize Sign...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   14|        212|             सेल डीड|सह दु.नि. अंधेरी 7|04/01/2023|1) पुष्पा बी रहेज...|1) Urn Dilip Sura...|1) सदनिका नं: 501...|https://pay2igr.i...|\n",
      "|   15|       7019|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|21/04/2023|1) मिशेल देसाई पू...|1) Attorney Geeta...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   16|       5868|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|12/04/2023|1) बांद्रा विनय क...|1) Krunal Sheth, ...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   17|       5450|              लीजडीड|सह दु.नि. अंधेरी 7|29/03/2023|1) दि एम आय जी को...|1) (Owner) Neelim...|1) इतर माहिती: टी...|https://pay2igr.i...|\n",
      "|   18|       3753|             सेल डीड|सह दु.नि. अंधेरी 7|01/03/2023|1) राहुल विनायक ड...|1) Rameshkumar Ma...|1) सदनिका नं: 801...|https://pay2igr.i...|\n",
      "|   19|       8517|66-नोटीस ऑफ़ लिस प...|सह दु.नि. अंधेरी 7|18/05/2023|1) व्हिज एंटरप्रा...|                    |1) सदनिका नं: 602...|https://pay2igr.i...|\n",
      "|   20|      12363|        59-हस्तांतरण|सह दु.नि. अंधेरी 7|11/07/2023|1) मयत ताराबेन शा...|1) Ku Mu Saryu Ap...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.withColumn('Will write down', translate_udf_og(spark_df['Will write down'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diarrhea type\n",
      "Du Prohibit Office\n",
      "Will write\n",
      "Other information\n"
     ]
    }
   ],
   "source": [
    "cols_to_translate = [\n",
    "    'diarrhea type',\n",
    "    'Du Prohibit Office',\n",
    "    'Will write', \n",
    "    # 'Will write down', \n",
    "    'Other information'\n",
    "    ]\n",
    "\n",
    "for column in cols_to_translate:\n",
    "    print(column)\n",
    "    # df_transformed = spark_df.withColumn(column, translate_udf(col(column)))\n",
    "    #spark_df.withColumn(column, translate_udf(spark_df[column])).show()\n",
    "    spark_df = spark_df.withColumn(column, translate_udf(spark_df[column]))\n",
    "\n",
    "spark_df = spark_df.withColumn('Will write down', translate_udf_og(spark_df['Will write down']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Sl no|Diarrhea no|       diarrhea type|Du Prohibit Office|      Year|          Will write|     Will write down|   Other information|           List no 2|\n",
      "+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|      13217|               lease|Co. D.N. Andheri 7|25/07/2023|1) Bhavin Sheth, ...|1) Hanmant Dhanwa...|1) Other informat...|https://pay2igr.i...|\n",
      "|    2|       4286|development agree...|Co. D.N. Andheri 7|10/03/2023|1) Ajit Singh Kar...|1) Paresh Ranchho...|1) Other informat...|https://pay2igr.i...|\n",
      "|    3|       2449|           sale deed|Co. D.N. Andheri 7|09/02/2023|1) Nupur Anil Kal...|1) Harsh Satpal M...|1) Other Informat...|https://pay2igr.i...|\n",
      "|    4|       8691|66-Notice of list...|Co. D.N. Andheri 7|22/05/2023|     1) Pradeep Soni|                    |1) Other Informat...|https://pay2igr.i...|\n",
      "|    5|       3551|65-correction letter|Co. D.N. Andheri 7|27/02/2023|1) Gunjan Yogeet ...|1) Shama Subhash ...|1) House No: 201,...|https://pay2igr.i...|\n",
      "|    6|       8043|development agree...|Co. D.N. Andheri 7|19/05/2023|1) Radhika Khande...|1) Suraj Kumar Do...|1) Other informat...|https://pay2igr.i...|\n",
      "|    7|       5454|               lease|Co. D.N. Andheri 7|29/03/2023|1) Planning was t...|1) Deputy Enginee...|1) House No.: Plo...|https://pay2igr.i...|\n",
      "|    8|       7732|    deed of transfer|Co. D.N. Andheri 7|04/05/2023|1) Chief Executiv...|1) Treasurer Ramy...|1) Other Details:...|https://pay2igr.i...|\n",
      "|    9|       5457|               lease|Co. D.N. Andheri 7|29/03/2023|1) Sai Dutt Prasa...|1) Ashok Kajane, ...|1) House No.: Plo...|https://pay2igr.i...|\n",
      "|   10|       2389|     promissory note|Co. D.N. Andheri 7|08/02/2023|1) Gandhi Nagar G...|                    |1) Other informat...|https://pay2igr.i...|\n",
      "|   11|       8896|65-correction letter|Co. D.N. Andheri 7|24/05/2023|1) . , Gopal Nara...|1) . , Sridhar Go...|1) Other informat...|https://pay2igr.i...|\n",
      "|   12|       8895|65-correction letter|Co. D.N. Andheri 7|24/05/2023|1) . , Gopal Nara...|1) . , Neha Gopal...|1) Other informat...|https://pay2igr.i...|\n",
      "|   13|       7415|development agree...|Co. D.N. Andheri 7|02/05/2023|1) सोसायटी मेंबर ...|1) Authorize Sign...|1) Other informat...|https://pay2igr.i...|\n",
      "|   14|        212|           sale deed|Co. D.N. Andheri 7|04/01/2023|1) Mukhtyar Rishi...|1) Urn Dilip Sura...|1) House No: 501,...|https://pay2igr.i...|\n",
      "|   15|       7019|development agree...|Co. D.N. Andheri 7|21/04/2023|1) Michel Desai P...|1) Attorney Geeta...|1) Other informat...|https://pay2igr.i...|\n",
      "|   16|       5868|development agree...|Co. D.N. Andheri 7|12/04/2023|1) Bandra Vinay C...|1) Krunal Sheth, ...|1) Other informat...|https://pay2igr.i...|\n",
      "|   17|       5450|           leasedeed|Co. D.N. Andheri 7|29/03/2023|1) Vijay Fatarfek...|1) (Owner) Neelim...|1) Other Informat...|https://pay2igr.i...|\n",
      "|   18|       3753|           sale deed|Co. D.N. Andheri 7|01/03/2023|1) Rahul Vinayak ...|1) Rameshkumar Ma...|1) House No: 801 ...|https://pay2igr.i...|\n",
      "|   19|       8517|66-Notice of list...|Co. D.N. Andheri 7|18/05/2023|1) Whiz Enterpris...|                    |1) House No: 602,...|https://pay2igr.i...|\n",
      "|   20|      12363|         59-transfer|Co. D.N. Andheri 7|11/07/2023|1) Ku Mu Saryu Ap...|1) Ku Mu Saryu Ap...|1) Other Informat...|https://pay2igr.i...|\n",
      "+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sl no: string (nullable = true)\n",
      " |-- Diarrhea no: string (nullable = true)\n",
      " |-- diarrhea type: string (nullable = true)\n",
      " |-- Du Prohibit Office: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Will write: string (nullable = true)\n",
      " |-- Will write down: string (nullable = true)\n",
      " |-- Other information: string (nullable = true)\n",
      " |-- List no 2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumnRenamed(existing='Will write',new='buyer_name')\n",
    "spark_df = spark_df.withColumnRenamed(existing='Will write down',new='seller_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sl no: string (nullable = true)\n",
      " |-- Diarrhea no: string (nullable = true)\n",
      " |-- diarrhea type: string (nullable = true)\n",
      " |-- Du Prohibit Office: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- buyer_name: string (nullable = true)\n",
      " |-- seller_name: string (nullable = true)\n",
      " |-- Other information: string (nullable = true)\n",
      " |-- List no 2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o831.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\winutils\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\winutils\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:607)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/spark_output/zipcodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o831.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\winutils\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\winutils\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:607)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n"
     ]
    }
   ],
   "source": [
    "spark_df.write.format(\"csv\").mode('overwrite').save(\"/tmp/spark_output/zipcodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_23996\\1038912717.py\", line 4, in translator_v2\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\googletrans\\client.py\", line 213, in translate\n    translated = ''.join([d[0] if d[0] else '' for d in data[0]])\nTypeError: 'NoneType' object is not iterable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 28\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# df_4 = new_df.select([\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#                       'Sl no',\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#                       'Year'\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#                       ]).toPandas()\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m df_5 \u001b[38;5;241m=\u001b[39m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSl no\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbuyer_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m---> 28\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m df_6 \u001b[38;5;241m=\u001b[39m new_df\u001b[38;5;241m.\u001b[39mselect([\n\u001b[0;32m     32\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSl no\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     33\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseller_name\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     34\u001b[0m                       ])\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1257\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_23996\\1038912717.py\", line 4, in translator_v2\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\googletrans\\client.py\", line 213, in translate\n    translated = ''.join([d[0] if d[0] else '' for d in data[0]])\nTypeError: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "# df_1 = new_df.select([\n",
    "#                       'Sl no', \n",
    "#                       'Diarrhea no'\n",
    "#                       ]).toPandas()\n",
    "\n",
    "print('1')\n",
    "# df_2 = new_df.select([\n",
    "#                       'Sl no', \n",
    "#                       'diarrhea type' \n",
    "#                       ]).toPandas()\n",
    "\n",
    "print('2')\n",
    "# df_3 = new_df.select([\n",
    "#                       'Sl no', \n",
    "#                       'Du Prohibit Office'\n",
    "#                      ]).toPandas()\n",
    "\n",
    "print('3')\n",
    "# df_4 = new_df.select([\n",
    "#                       'Sl no',\n",
    "#                       'Year'\n",
    "#                       ]).toPandas()\n",
    "\n",
    "print('4')\n",
    "df_5 = spark_df.select([\n",
    "                      'Sl no',\n",
    "                      'buyer_name',\n",
    "                      ]).toPandas()\n",
    "\n",
    "print('5')\n",
    "df_6 = new_df.select([\n",
    "                      'Sl no',\n",
    "                      'seller_name',\n",
    "                      ]).toPandas()\n",
    "\n",
    "print('6')\n",
    "df_7 = new_df.select([\n",
    "                      'Sl no',\n",
    "                      'Other information',\n",
    "                      ]).toPandas()\n",
    "print('7')\n",
    "df_8 = new_df.select([\n",
    "                      'Sl no',\n",
    "                      'List no 2',\n",
    "                      ]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_23996\\1038912717.py\", line 4, in translator_v2\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\googletrans\\client.py\", line 213, in translate\n    translated = ''.join([d[0] if d[0] else '' for d in data[0]])\nTypeError: 'NoneType' object is not iterable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m check_df \u001b[38;5;241m=\u001b[39m \u001b[43mnew_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSl no\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDiarrhea no\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiarrhea type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseller_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDu Prohibit Office\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbuyer_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseller_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOther information\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m----> 2\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mList no 2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1257\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_23996\\1038912717.py\", line 4, in translator_v2\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\googletrans\\client.py\", line 213, in translate\n    translated = ''.join([d[0] if d[0] else '' for d in data[0]])\nTypeError: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "check_df = new_df.select(['Sl no', 'Diarrhea no', 'diarrhea type', 'seller_name', 'Du Prohibit Office','Year','buyer_name','seller_name','Other information',\n",
    "                          'List no 2']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seller_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1) Hanmant Dhanware, Executive Engineer, Mumba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1) Paresh Ranchhod Patel, partner of Navish Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1) Harsh Satpal Malhotra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1) Shama Subhash Oberoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>1) Muhammad Kunhi KC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>1) Rashmi Satish Kevalramani\\n2) Satish Kevalr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>1) Jeram Chamadia (HUF) Laxman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>1) Karim B. pasture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>1) Mr. Pratyush Bharatiya, Partner by Saffron ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1209 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            seller_name\n",
       "0     1) Hanmant Dhanware, Executive Engineer, Mumba...\n",
       "1     1) Paresh Ranchhod Patel, partner of Navish Re...\n",
       "2                              1) Harsh Satpal Malhotra\n",
       "3                                                      \n",
       "4                               1) Shama Subhash Oberoi\n",
       "...                                                 ...\n",
       "1204                               1) Muhammad Kunhi KC\n",
       "1205  1) Rashmi Satish Kevalramani\\n2) Satish Kevalr...\n",
       "1206                     1) Jeram Chamadia (HUF) Laxman\n",
       "1207                                1) Karim B. pasture\n",
       "1208  1) Mr. Pratyush Bharatiya, Partner by Saffron ...\n",
       "\n",
       "[1209 rows x 1 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o444.save.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 18\u001b[0m\n\u001b[0;32m      3\u001b[0m jdbc_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:postgresql://127.0.0.1:5432/propReturns\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your PostgreSQL connection details\u001b[39;00m\n\u001b[0;32m      4\u001b[0m properties \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m,    \u001b[38;5;66;03m# Replace with your PostgreSQL username\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSunrise12345\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Replace with your PostgreSQL password\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o444.save.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a table in PostgreSQL\n",
    "table_name = \"record_details_translated\"  # Replace with your desired table name\n",
    "jdbc_url = \"jdbc:postgresql://127.0.0.1:5432/propReturns\"  # Replace with your PostgreSQL connection details\n",
    "properties = {\n",
    "    \"user\": \"postgres\",    # Replace with your PostgreSQL username\n",
    "    \"password\": \"Sunrise12345\",  # Replace with your PostgreSQL password\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "spark_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"]) \\\n",
    "    .option(\"driver\", properties[\"driver\"]) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 DataFrame has been inserted into the 'record_details_translated' table.\n"
     ]
    }
   ],
   "source": [
    "table_name = 'record_details_translated'\n",
    "records_translate.to_sql(table_name, engine, if_exists='replace', index=True)\n",
    "print(f\"{len(records_translate)} DataFrame has been inserted into the '{table_name}' table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
