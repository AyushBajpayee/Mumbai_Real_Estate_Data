{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.selenium_manager import SeleniumManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "from googletrans import Translator\n",
    "from google_trans_new import google_translator  \n",
    "from deep_translator import GoogleTranslator\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "import pyspark.pandas as ps\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def translator(s,source):\n",
    "    return GoogleTranslator(source=source, target='en').translate(s)\n",
    "\n",
    "\n",
    "def translator_v2(s,source='auto'):\n",
    "        translator = Translator()\n",
    "        return translator.translate(s, src='hi',dest='en').text if s != '' else 'null' \n",
    "\n",
    "translate_udf = udf(translator_v2,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbselect 2023\n",
      "Clicked 2023 for dbselect\n",
      "district_id मुंबई उपनगर\n",
      "Clicked मुंबई उपनगर for district_id\n",
      "taluka_id अंधेरी\n",
      "Clicked अंधेरी for taluka_id\n",
      "village_id बांद्रा\n",
      "Clicked बांद्रा for village_id\n",
      "Entered Captcha JCIW9D\n",
      "Set to All Records\n",
      "So Far, So Good\n"
     ]
    }
   ],
   "source": [
    "#selenium webdriver\n",
    "input_params_dropdown_by_id = {}\n",
    "input_params_dropdown_by_id['dbselect'] = '2023' #Select Year\n",
    "input_params_dropdown_by_id['district_id'] = 'मुंबई उपनगर' #District\n",
    "input_params_dropdown_by_id['taluka_id'] = 'अंधेरी' #Taluka\n",
    "input_params_dropdown_by_id['village_id'] = 'बांद्रा' #Village\n",
    "\n",
    "input_params_text_by_id = {}\n",
    "input_params_text_by_id['free_text'] = '2023' \n",
    "\n",
    "url = 'https://pay2igr.igrmaharashtra.gov.in/eDisplay/Propertydetails/index' \n",
    "driver = webdriver.Chrome() \n",
    "driver.implicitly_wait(10)\n",
    "driver.get(url)\n",
    "\n",
    "for key, value in input_params_dropdown_by_id.items():\n",
    "\tprint(key,value)\n",
    "\tdropdown = driver.find_element(by=By.ID,value=key)\n",
    "\tdropdown_select = Select(dropdown)\n",
    "\tfor option in dropdown_select.options:\n",
    "\t\tif option.text == value:\n",
    "\t\t\toption.click()\n",
    "\t\t\tprint(f'Clicked {option.text} for {key}')\n",
    "\t\t\tbreak\n",
    "\tdriver.implicitly_wait(2)\n",
    "\n",
    "#Input reg year\n",
    "driver.find_element(by=By.ID, value='free_text').send_keys(input_params_text_by_id['free_text'])\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "#Input Captcha\n",
    "captcha = input()\n",
    "driver.find_element(by=By.ID, value='cpatchaTextBox').send_keys(captcha)\n",
    "print(f'Entered Captcha {captcha}')\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "#Submit to get result set\n",
    "driver.find_element(by=By.ID,value='submit').click()\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "\n",
    "#Click on 50 Pages\n",
    "dropdown = driver.find_element(by=By.NAME, value='tableparty_length')\n",
    "dropdown_select = Select(dropdown)\n",
    "for option in dropdown_select.options:\n",
    "\tif option.text == 'All':\n",
    "\t\toption.click()\n",
    "\t\tprint('Set to All Records')\n",
    "\t\tbreak\n",
    "\n",
    "print('So Far, So Good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#scrap records\u001b[39;00m\n\u001b[0;32m      2\u001b[0m records_raw \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mअनु क्र.\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mदस्त क्र.\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mदस्त प्रकार\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mदू. नि. कार्यालय\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mवर्ष\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mलिहून देणार\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mलिहून घेणार\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mइतर माहीती\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mसूची क्र. २\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdriver\u001b[49m\u001b[38;5;241m.\u001b[39mfind_elements(by\u001b[38;5;241m=\u001b[39mBy\u001b[38;5;241m.\u001b[39mID, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtbdata\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m      5\u001b[0m     data \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mसूची क्र. २\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m item\u001b[38;5;241m.\u001b[39mfind_element(by\u001b[38;5;241m=\u001b[39mBy\u001b[38;5;241m.\u001b[39mTAG_NAME,value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_elements(by\u001b[38;5;241m=\u001b[39mBy\u001b[38;5;241m.\u001b[39mXPATH, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.//*[self::td or self::th]\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m      6\u001b[0m     records_raw\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(records_raw)] \u001b[38;5;241m=\u001b[39m data\n",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "#scrap records\n",
    "records_raw = pd.DataFrame(columns=['अनु क्र.','दस्त क्र.','दस्त प्रकार','दू. नि. कार्यालय','वर्ष','लिहून देणार','लिहून घेणार','इतर माहीती','सूची क्र. २'])\n",
    "\n",
    "for index, table in enumerate(driver.find_elements(by=By.ID, value='tbdata')):\n",
    "    data = [item.text if item.text != 'सूची क्र. २' else item.find_element(by=By.TAG_NAME,value='a').get_attribute('href') for item in table.find_elements(by=By.XPATH, value=\".//*[self::td or self::th]\")]\n",
    "    records_raw.loc[len(records_raw)] = data\n",
    "    # print(data)\n",
    "    print(f'{index} Row appended')\n",
    "\n",
    "records_raw\n",
    "records_translate = records_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209 DataFrame has been inserted into the 'record_details_raw' table.\n"
     ]
    }
   ],
   "source": [
    "# Insert Scraped Raw Data in postgres\n",
    "db_user = 'postgres'\n",
    "db_password = 'Sunrise12345'\n",
    "db_host = '127.0.0.1'\n",
    "db_port = '5432'\n",
    "db_name = 'propReturns'\n",
    "\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "table_name = 'record_details_raw'\n",
    "\n",
    "records_raw.to_sql(table_name, engine, if_exists='replace', index=True)\n",
    "\n",
    "print(f\"{len(records_raw)} DataFrame has been inserted into the '{table_name}' table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [0,100]\n",
      "Translating diarrhea type\n",
      "Done\n",
      "Translating Du. Prohibit. Office\n",
      "Done\n",
      "Translating Will write\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "Request exception can happen due to an api connection error. Please check your connection and try again",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_to_translate:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     temp_df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m records_translated \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([records_translated,temp_df])\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_to_translate:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     temp_df[col] \u001b[38;5;241m=\u001b[39m temp_df[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m records_translated \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([records_translated,temp_df])\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mtranslator\u001b[1;34m(s, source)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslator\u001b[39m(s,source):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGoogleTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\deep_translator\\google.py:74\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_failed(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code):\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError()\n\u001b[0;32m     76\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m element \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_tag, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_query)\n",
      "\u001b[1;31mRequestError\u001b[0m: Request exception can happen due to an api connection error. Please check your connection and try again"
     ]
    }
   ],
   "source": [
    "#Approach Via Pandas --> Extremely Slow\n",
    "records_translate = records_raw.copy()\n",
    "records_translate.columns = [translator(col,source='auto') for col in records_translate.columns]\n",
    "cols_to_translate = [\n",
    "    'diarrhea type',\n",
    "    'Du. Prohibit. Office',\n",
    "    'Will write', \n",
    "    'Will write down', \n",
    "    'Other information'\n",
    "    ]\n",
    "\n",
    "records_translated = pd.DataFrame(columns=records_translate.columns)\n",
    "batch_size = 100\n",
    "start = 0\n",
    "for batch in range(start,len(records_translate),batch_size):\n",
    "    print(f'Batch -> [{batch},{batch+batch_size}]')\n",
    "    temp_df = records_translate.iloc[batch:batch+batch_size,:].copy()\n",
    "    for col in cols_to_translate:\n",
    "        print(f'Translating {col}')\n",
    "        temp_df[col] = temp_df[col].apply(lambda x: translator(x,source='hi'))\n",
    "        print(f'Done')\n",
    "    records_translated = pd.concat([records_translated,temp_df])\n",
    "records_translated.rename(columns={'Will write':'buyer_name','Will write down':'seller_name'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del os.environ['PYSPARK_PYTHON'] \n",
    "# del os.environ['PYSPARK_DRIVER_PYTHON'] \n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable \n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m spark\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "del spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20b465bbe80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach Via Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# spark = SparkSession.builder \\\n",
    "#         .config('spark.executor.instances', 4) \\\n",
    "#         .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "#         .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "#         .config(\"spark.dynamicAllocation.minExecutors\",\"1\") \\\n",
    "#         .config(\"spark.dynamicAllocation.maxExecutors\",\"5\") \\\n",
    "#         .config(\"spark.executor.cores\",6) \\\n",
    "#         .config(\"spark.executor.memory\",\"2g\") \\\n",
    "#         .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "# #spark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.executor.cores\", 4)\n",
    "# spark.conf.set(\"spark.dynamicAllocation.minExecutors\",\"1\")\n",
    "# spark.conf.set(\"spark.dynamicAllocation.maxExecutors\",\"5\")\n",
    "# conf = spark.sparkContext.getConf()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Data from SQL directly\n",
    "host = '127.0.0.1'\n",
    "port = '5432'\n",
    "database = 'propReturns'\n",
    "username = 'postgres'\n",
    "password = 'Sunrise12345'\n",
    "url = f\"jdbc:postgresql://{host}:{port}/{database}\"\n",
    "table_name = 'record_details_raw'\n",
    "\n",
    "engine = create_engine(f'postgresql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "spark_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password).load()\n",
    "\n",
    "records_raw = pd.read_sql('record_details_raw',engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for col in records_raw.columns:\n",
    "    new_names.append(translator(col,source='auto'))\n",
    "\n",
    "new_names = [x.replace('.','') for x in new_names]\n",
    "spark_df = spark_df.toDF(*new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|index|Sl no|Diarrhea no|       diarrhea type|Du Prohibit Office|      Year|          Will write|     Will write down|   Other information|           List no 2|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|    1|       4286|       विकसनकरारनामा|सह दु.नि. अंधेरी 7|10/03/2023|1) अजित सिंह करता...|1) नाविश रियल्टी ...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|    1|    2|      13217|           भाडेपट्टा|सह दु.नि. अंधेरी 7|25/07/2023|1) श्रीमती चंद्रक...|1) महाराष्ट़ हाऊस...|1) इतर माहिती: पि...|https://pay2igr.i...|\n",
      "|    2|    3|       2449|             सेल डीड|सह दु.नि. अंधेरी 7|09/02/2023|1) नूपुर अनिल कळक...|1) हर्ष सतपाल मल्...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|    3|    4|       8691|66-नोटीस ऑफ़ लिस प...|सह दु.नि. अंधेरी 7|22/05/2023|      1) प्रदीप सोनी|                    |1) इतर माहिती: सि...|https://pay2igr.i...|\n",
      "|    4|    5|       3551|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|27/02/2023|1) गुंजन योगीत कप...|1) शामा सुभाष ओबेराय|1) सदनिका नं: 201...|https://pay2igr.i...|\n",
      "|    5|    6|       8043|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|19/05/2023|1) राधिका खंडेलवा...|1) व्हीबीएचडीसी ब...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   12|   13|       7415|       विकसनकरारनामा|सह दु.नि. अंधेरी 7|02/05/2023|1) सोसायटी मेंबर ...|1) इंस्पायरा बिल्...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|    6|    7|       5454|           भाडेपट्टा|सह दु.नि. अंधेरी 7|29/03/2023|1) नियोजन को ऑपरे...|1) मुंबई हाऊसिंग ...|1) सदनिका नं: प्ल...|https://pay2igr.i...|\n",
      "|    7|    8|       5457|           भाडेपट्टा|सह दु.नि. अंधेरी 7|29/03/2023|1) साई दत्त प्रसा...|1) मुंबई हाऊसिंग ...|1) सदनिका नं: प्ल...|https://pay2igr.i...|\n",
      "|    8|    9|       7732|    अभिहस्तांतरणपत्र|सह दु.नि. अंधेरी 7|04/05/2023|1) मे एकता सुप्री...|1) युडोरा कॉ ऑप ह...|1) इतर माहिती: इम...|https://pay2igr.i...|\n",
      "|    9|   10|       8895|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|24/05/2023|1) . . गोपाल नारा...|1) . . नेहा गोपाल...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|   10|   11|       8896|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|24/05/2023|1) . . गोपाल नारा...|1) . . श्रीधर गोप...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|   11|   12|       2389|       प्रतिज्ञापत्र|सह दु.नि. अंधेरी 7|08/02/2023|1) गांघी नगर गणेश...|                    |1) इतर माहिती: re...|https://pay2igr.i...|\n",
      "|   13|   14|        212|             सेल डीड|सह दु.नि. अंधेरी 7|04/01/2023|1) पुष्पा बी रहेज...|1) कलश दिलीप सुरा...|1) सदनिका नं: 501...|https://pay2igr.i...|\n",
      "|   14|   15|       7019|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|21/04/2023|1) मिशेल देसाई पू...|1) किऑर्बिट रिअल्...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   15|   16|       5868|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|12/04/2023|1) बांद्रा विनय क...|1) डायनास्टी इन्फ...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   16|   17|       5450|              लीजडीड|सह दु.नि. अंधेरी 7|29/03/2023|1) दि एम आय जी को...|1) (मालक ) महाराष...|1) इतर माहिती: टी...|https://pay2igr.i...|\n",
      "|   17|   18|       3753|             सेल डीड|सह दु.नि. अंधेरी 7|01/03/2023|1) राहुल विनायक ड...|1) रमेशकुमार मानक...|1) सदनिका नं: 801...|https://pay2igr.i...|\n",
      "|   18|   19|       8517|66-नोटीस ऑफ़ लिस प...|सह दु.नि. अंधेरी 7|18/05/2023|1) व्हिज एंटरप्रा...|                    |1) सदनिका नं: 602...|https://pay2igr.i...|\n",
      "|   33|   34|       3172| लिव्ह अॅड लायसन्सेस|सह दु.नि. अंधेरी 7|20/02/2023|     1) बिमला देवी .|1) पूरीबेन बच्चू ...|1) फ़्लॅट नं:प्लाट...|https://pay2igr.i...|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_translate = [\n",
    "    'diarrhea type',\n",
    "    'Du Prohibit Office',\n",
    "    'Will write', \n",
    "    'Will write down', \n",
    "    'Other information'\n",
    "    ]\n",
    "\n",
    "for column in cols_to_translate:\n",
    "    spark_df = spark_df.withColumn(column, translate_udf(spark_df[column]))\n",
    "\n",
    "spark_df = spark_df.withColumnRenamed(existing='Will write',new='buyer_name')\n",
    "spark_df = spark_df.withColumnRenamed(existing='Will write down',new='seller_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|index|Sl no|Diarrhea no|       diarrhea type|Du Prohibit Office|      Year|          buyer_name|         seller_name|   Other information|           List no 2|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|    1|       4286|development agree...|Co. D.N. Andheri 7|10/03/2023|1) Ajit Singh Kar...|1) Paresh Ranchho...|1) Other informat...|https://pay2igr.i...|\n",
      "|    1|    2|      13217|               lease|Co. D.N. Andheri 7|25/07/2023|1) Bhavin Sheth, ...|1) Hanmant Dhanva...|1) Other informat...|https://pay2igr.i...|\n",
      "|    2|    3|       2449|           sale deed|Co. D.N. Andheri 7|09/02/2023|1) Nupur Anil Kal...|1) Harsh Satpal M...|1) Other Informat...|https://pay2igr.i...|\n",
      "|    3|    4|       8691|66-Notice of list...|Co. D.N. Andheri 7|22/05/2023|     1) Pradeep Soni|                null|1) Other Informat...|https://pay2igr.i...|\n",
      "|    4|    5|       3551|65-correction letter|Co. D.N. Andheri 7|27/02/2023|1) Gunjan Yogeet ...|1) Shama Subhash ...|1) House No: 201,...|https://pay2igr.i...|\n",
      "|    5|    6|       8043|development agree...|Co. D.N. Andheri 7|19/05/2023|1) Radhika Khande...|1) Suraj Kumar Du...|1) Other informat...|https://pay2igr.i...|\n",
      "|   12|   13|       7415|development agree...|Co. D.N. Andheri 7|02/05/2023|1) सोसायटी मेंबर ...|1) Inspira Buildc...|1) Other informat...|https://pay2igr.i...|\n",
      "|    6|    7|       5454|               lease|Co. D.N. Andheri 7|29/03/2023|1) Planning was t...|1) T R Chatupule,...|1) House No.: Plo...|https://pay2igr.i...|\n",
      "|    7|    8|       5457|               lease|Co. D.N. Andheri 7|29/03/2023|1) Sai Dutt Prasa...|1) Ashok Kajne, E...|1) House No.: Plo...|https://pay2igr.i...|\n",
      "|    8|    9|       7732|    deed of transfer|Co. D.N. Andheri 7|04/05/2023|1) Chief Executiv...|1) Khazindar Rama...|1) Other Details:...|https://pay2igr.i...|\n",
      "|    9|   10|       8895|65-correction letter|Co. D.N. Andheri 7|24/05/2023|1) . , Gopal Nara...|1) . , Neha Gopal...|1) Other informat...|https://pay2igr.i...|\n",
      "|   10|   11|       8896|65-correction letter|Co. D.N. Andheri 7|24/05/2023|1) . , Gopal Nara...|1) . , Sridhar Go...|1) Other informat...|https://pay2igr.i...|\n",
      "|   11|   12|       2389|     promissory note|Co. D.N. Andheri 7|08/02/2023|1) Gandhi Nagar G...|                null|1) Other informat...|https://pay2igr.i...|\n",
      "|   13|   14|        212|           sale deed|Co. D.N. Andheri 7|04/01/2023|1) Mukhtyar Rishi...|1) Kalash Dilip S...|1) House No: 501,...|https://pay2igr.i...|\n",
      "|   14|   15|       7019|development agree...|Co. D.N. Andheri 7|21/04/2023|1) Michel Desai P...|1) Chief Geeta Mo...|1) Other informat...|https://pay2igr.i...|\n",
      "|   15|   16|       5868|development agree...|Co. D.N. Andheri 7|12/04/2023|1) Chairman Jaywa...|1) Krunal Sheth, ...|1) Other informat...|https://pay2igr.i...|\n",
      "|   16|   17|       5450|           leasedeed|Co. D.N. Andheri 7|29/03/2023|1) Vijay Fatarfek...|1) (Owner) Neelim...|1) Other Informat...|https://pay2igr.i...|\n",
      "|   17|   18|       3753|           sale deed|Co. D.N. Andheri 7|01/03/2023|1) Rahul Vinayak ...|1) Rameshkumar Ma...|1) House No: 801 ...|https://pay2igr.i...|\n",
      "|   18|   19|       8517|66-Notice of list...|Co. D.N. Andheri 7|18/05/2023|1) Whiz Enterpris...|                null|1) House No: 602,...|https://pay2igr.i...|\n",
      "|   33|   34|       3172|    live ad licenses|Co. D.N. Andheri 7|20/02/2023|      1) Bimla Devi.|1) Puriben Bachch...|1) Flat No: Plot ...|https://pay2igr.i...|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_sink = 'record_details_translated'\n",
    "\n",
    "spark_df.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", table_name_sink) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_translated = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_16296\\3257740425.py\", line 8, in translator_v2\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\googletrans\\client.py\", line 210, in translate\n    data, response = self._translate(text, dest, src, kwargs)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\googletrans\\client.py\", line 108, in _translate\n    r = self.client.get(url, params=params)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 755, in get\n    return self.request(\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 600, in request\n    return self.send(\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 620, in send\n    response = self.send_handling_redirects(\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 647, in send_handling_redirects\n    response = self.send_handling_auth(\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 684, in send_handling_auth\n    response = self.send_single_request(request, timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 721, in send_single_request\n    except HTTPError as exc:\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 160, in request\n    raise\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 78, in request\n    return self.connection.request(method, url, headers, stream, timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py\", line 121, in request\n    raise\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py\", line 292, in request\n    status_code, headers = self.receive_response(timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py\", line 344, in receive_response\n    event = self.connection.wait_for_event(self.stream_id, timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py\", line 197, in wait_for_event\n    self.receive_events(timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py\", line 204, in receive_events\n    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_backends\\sync.py\", line 60, in read\n    with map_exceptions(exc_map):\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\contextlib.py\", line 183, in __exit__\n    raise\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_exceptions.py\", line 12, in map_exceptions\n    raise to_exc(exc) from None\nhttpcore._exceptions.ReadTimeout: The read operation timed out\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m records_translated \u001b[38;5;241m=\u001b[39m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:131\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\n\u001b[0;32m    130\u001b[0m self_destruct \u001b[38;5;241m=\u001b[39m jconf\u001b[38;5;241m.\u001b[39marrowPySparkSelfDestructEnabled()\n\u001b[1;32m--> 131\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_as_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_destruct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    133\u001b[0m     table \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(batches)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:282\u001b[0m, in \u001b[0;36mPandasConversionMixin._collect_as_arrow\u001b[1;34m(self, split_batches)\u001b[0m\n\u001b[0;32m    280\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(batch_stream)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_spark_exception():\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;66;03m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[39;00m\n\u001b[0;32m    284\u001b[0m         jsocket_auth_server\u001b[38;5;241m.\u001b[39mgetResult()\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Separate RecordBatches from batch order indices in results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:204\u001b[0m, in \u001b[0;36munwrap_spark_exception\u001b[1;34m()\u001b[0m\n\u001b[0;32m    202\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(je\u001b[38;5;241m.\u001b[39mgetCause())\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_16296\\3257740425.py\", line 8, in translator_v2\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\googletrans\\client.py\", line 210, in translate\n    data, response = self._translate(text, dest, src, kwargs)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\googletrans\\client.py\", line 108, in _translate\n    r = self.client.get(url, params=params)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 755, in get\n    return self.request(\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 600, in request\n    return self.send(\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 620, in send\n    response = self.send_handling_redirects(\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 647, in send_handling_redirects\n    response = self.send_handling_auth(\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 684, in send_handling_auth\n    response = self.send_single_request(request, timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpx\\_client.py\", line 721, in send_single_request\n    except HTTPError as exc:\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 160, in request\n    raise\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection.py\", line 78, in request\n    return self.connection.request(method, url, headers, stream, timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py\", line 121, in request\n    raise\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py\", line 292, in request\n    status_code, headers = self.receive_response(timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py\", line 344, in receive_response\n    event = self.connection.wait_for_event(self.stream_id, timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py\", line 197, in wait_for_event\n    self.receive_events(timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_sync\\http2.py\", line 204, in receive_events\n    data = self.socket.read(self.READ_NUM_BYTES, timeout)\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_backends\\sync.py\", line 60, in read\n    with map_exceptions(exc_map):\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\contextlib.py\", line 183, in __exit__\n    raise\n  File \"c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\httpcore\\_exceptions.py\", line 12, in map_exceptions\n    raise to_exc(exc) from None\nhttpcore._exceptions.ReadTimeout: The read operation timed out\n"
     ]
    }
   ],
   "source": [
    "records_translated = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209 DataFrame has been inserted into the 'record_details_translated' table.\n"
     ]
    }
   ],
   "source": [
    "records_translated = spark_df.toPandas()\n",
    "table_name = 'record_details_translated'\n",
    "records_translated.to_sql(table_name, engine, if_exists='replace', index=True)\n",
    "print(f\"{len(records_translate)} DataFrame has been inserted into the '{table_name}' table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
