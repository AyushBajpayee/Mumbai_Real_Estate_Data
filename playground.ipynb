{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1ST PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.selenium_manager import SeleniumManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "from googletrans import Translator\n",
    "from google_trans_new import google_translator  \n",
    "from deep_translator import GoogleTranslator\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from tqdm import tqdm\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pdfplumber\n",
    "\n",
    "import subprocess\n",
    "import pathlib\n",
    "import ghostscript as gs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def translator(s,source):\n",
    "    return GoogleTranslator(source=source, target='en').translate(s)\n",
    "\n",
    "\n",
    "def translator_v2(s,source='auto'):\n",
    "        translator = Translator()\n",
    "        return translator.translate(s, src='hi',dest='en').text if s != '' else 'null' \n",
    "\n",
    "translate_udf = udf(translator_v2,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbselect 2023\n",
      "Clicked 2023 for dbselect\n",
      "district_id मुंबई उपनगर\n",
      "Clicked मुंबई उपनगर for district_id\n",
      "taluka_id अंधेरी\n",
      "Clicked अंधेरी for taluka_id\n",
      "village_id बांद्रा\n",
      "Clicked बांद्रा for village_id\n",
      "Entered Captcha LE291J\n",
      "Set to All Records\n",
      "So Far, So Good\n"
     ]
    }
   ],
   "source": [
    "#selenium webdriver\n",
    "input_params_dropdown_by_id = {}\n",
    "input_params_dropdown_by_id['dbselect'] = '2023' #Select Year\n",
    "input_params_dropdown_by_id['district_id'] = 'मुंबई उपनगर' #District\n",
    "input_params_dropdown_by_id['taluka_id'] = 'अंधेरी' #Taluka\n",
    "input_params_dropdown_by_id['village_id'] = 'बांद्रा' #Village\n",
    "\n",
    "input_params_text_by_id = {}\n",
    "input_params_text_by_id['free_text'] = '2023' \n",
    "\n",
    "url = 'https://pay2igr.igrmaharashtra.gov.in/eDisplay/Propertydetails/index' \n",
    "driver = webdriver.Chrome() \n",
    "driver.implicitly_wait(10)\n",
    "driver.get(url)\n",
    "\n",
    "for key, value in input_params_dropdown_by_id.items():\n",
    "\tprint(key,value)\n",
    "\tdropdown = driver.find_element(by=By.ID,value=key)\n",
    "\tdropdown_select = Select(dropdown)\n",
    "\tfor option in dropdown_select.options:\n",
    "\t\tif option.text == value:\n",
    "\t\t\toption.click()\n",
    "\t\t\tprint(f'Clicked {option.text} for {key}')\n",
    "\t\t\tbreak\n",
    "\tdriver.implicitly_wait(2)\n",
    "\n",
    "#Input reg year\n",
    "driver.find_element(by=By.ID, value='free_text').send_keys(input_params_text_by_id['free_text'])\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "#Input Captcha\n",
    "captcha = input()\n",
    "driver.find_element(by=By.ID, value='cpatchaTextBox').send_keys(captcha)\n",
    "print(f'Entered Captcha {captcha}')\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "#Submit to get result set\n",
    "driver.find_element(by=By.ID,value='submit').click()\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "\n",
    "#Click on 50 Pages\n",
    "dropdown = driver.find_element(by=By.NAME, value='tableparty_length')\n",
    "dropdown_select = Select(dropdown)\n",
    "for option in dropdown_select.options:\n",
    "\tif option.text == 'All':\n",
    "\t\toption.click()\n",
    "\t\tprint('Set to All Records')\n",
    "\t\tbreak\n",
    "\n",
    "print('So Far, So Good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1209/1209 [03:09<00:00,  6.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#scrap records\n",
    "records_raw = pd.DataFrame(columns=['अनु क्र.','दस्त क्र.','दस्त प्रकार','दू. नि. कार्यालय','वर्ष','लिहून देणार','लिहून घेणार','इतर माहीती','सूची क्र. २'])\n",
    "\n",
    "print('Data Loading...')\n",
    "for index, table in enumerate(tqdm(driver.find_elements(by=By.ID, value='tbdata'))):\n",
    "    data = [item.text if item.text != 'सूची क्र. २' else item.find_element(by=By.TAG_NAME,value='a').get_attribute('href') for item in table.find_elements(by=By.XPATH, value=\".//*[self::td or self::th]\")]\n",
    "    records_raw.loc[len(records_raw)] = data\n",
    "    # print(data)\n",
    "    # print(f'{index} Row appended',)\n",
    "\n",
    "print('Finished!')\n",
    "records_raw\n",
    "records_translate = records_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209 DataFrame has been inserted into the 'record_details_raw' table.\n"
     ]
    }
   ],
   "source": [
    "# Insert Scraped Raw Data in postgres\n",
    "db_user = 'postgres'\n",
    "db_password = 'Sunrise12345'\n",
    "db_host = '127.0.0.1'\n",
    "db_port = '5432'\n",
    "db_name = 'propReturns'\n",
    "\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "table_name = 'record_details_raw'\n",
    "\n",
    "records_raw.to_sql(table_name, engine, if_exists='replace', index=True)\n",
    "\n",
    "print(f\"{len(records_raw)} DataFrame has been inserted into the '{table_name}' table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [0,100]\n",
      "Translating diarrhea type\n",
      "Done\n",
      "Translating Du. Prohibit. Office\n",
      "Done\n",
      "Translating Will write\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "Request exception can happen due to an api connection error. Please check your connection and try again",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_to_translate:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     temp_df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m records_translated \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([records_translated,temp_df])\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_to_translate:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     temp_df[col] \u001b[38;5;241m=\u001b[39m temp_df[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m records_translated \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([records_translated,temp_df])\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mtranslator\u001b[1;34m(s, source)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslator\u001b[39m(s,source):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGoogleTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\deep_translator\\google.py:74\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_failed(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code):\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError()\n\u001b[0;32m     76\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m element \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_tag, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_query)\n",
      "\u001b[1;31mRequestError\u001b[0m: Request exception can happen due to an api connection error. Please check your connection and try again"
     ]
    }
   ],
   "source": [
    "#Approach Via Pandas --> Extremely Slow\n",
    "records_translate = records_raw.copy()\n",
    "records_translate.columns = [translator(col,source='auto') for col in records_translate.columns]\n",
    "cols_to_translate = [\n",
    "    'diarrhea type',\n",
    "    'Du. Prohibit. Office',\n",
    "    'Will write', \n",
    "    'Will write down', \n",
    "    'Other information'\n",
    "    ]\n",
    "\n",
    "records_translated = pd.DataFrame(columns=records_translate.columns)\n",
    "batch_size = 100\n",
    "start = 0\n",
    "for batch in range(start,len(records_translate),batch_size):\n",
    "    print(f'Batch -> [{batch},{batch+batch_size}]')\n",
    "    temp_df = records_translate.iloc[batch:batch+batch_size,:].copy()\n",
    "    for col in cols_to_translate:\n",
    "        print(f'Translating {col}')\n",
    "        temp_df[col] = temp_df[col].apply(lambda x: translator(x,source='hi'))\n",
    "        print(f'Done')\n",
    "    records_translated = pd.concat([records_translated,temp_df])\n",
    "records_translated.rename(columns={'Will write':'buyer_name','Will write down':'seller_name'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_translate = records_raw.copy()\n",
    "records_translate.columns = [translator(col,source='auto') for col in records_translate.columns]\n",
    "cols_to_translate = [\n",
    "    'diarrhea type',\n",
    "    'Du. Prohibit. Office',\n",
    "    'Will write', \n",
    "    'Will write down', \n",
    "    'Other information'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Sl no.', 'Diarrhea no.', 'diarrhea type',\n",
       "       'Du. Prohibit. Office', 'Year', 'Will write', 'Will write down',\n",
       "       'Other information', 'List no. 2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_translate.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_translate['to_translate'] = records_translate['diarrhea type'] + '__$__' \\\n",
    "                                    + records_translate['Du. Prohibit. Office'] + '__$__' \\\n",
    "                                    + records_translate['Will write'] + '__$__' \\\n",
    "                                    + records_translate['Will write down'] + '__$__' \\\n",
    "                                    + records_translate['Other information']  \n",
    "                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1209/1209 [07:33<00:00,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "records_translated = pd.DataFrame(columns=records_translate.columns)\n",
    "batch_size = 1\n",
    "start = 0\n",
    "for batch in tqdm(range(start,len(records_translate))):\n",
    "    # print(f'Batch -> [{batch},{batch+batch_size}]')\n",
    "    temp_df = records_translate.iloc[batch:batch+batch_size,:].copy()\n",
    "    temp_df['to_translate'] = temp_df['to_translate'].apply(translator_v2)\n",
    "    # for col in cols_to_translate:\n",
    "    #     # print(f'Translating {col}')\n",
    "    #     temp_df[col] = temp_df[col].apply(lambda x: translator(x,source='hi'))\n",
    "    #     # print(f'Done')\n",
    "    records_translated = pd.concat([records_translated,temp_df])\n",
    "records_translated.rename(columns={'Will write':'buyer_name','Will write down':'seller_name'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_translated_og = records_translated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_translate = [\n",
    "    'diarrhea type',\n",
    "    'Du. Prohibit. Office',\n",
    "    'buyer_name', \n",
    "    'seller_name', \n",
    "    'Other information'\n",
    "    ]\n",
    "\n",
    "records_translated[cols_to_translate] = records_translated['to_translate'].apply(lambda x: pd.Series(x.split('__$__')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Sl no.</th>\n",
       "      <th>Diarrhea no.</th>\n",
       "      <th>diarrhea type</th>\n",
       "      <th>Du. Prohibit. Office</th>\n",
       "      <th>Year</th>\n",
       "      <th>buyer_name</th>\n",
       "      <th>seller_name</th>\n",
       "      <th>Other information</th>\n",
       "      <th>List no. 2</th>\n",
       "      <th>to_translate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13217</td>\n",
       "      <td>Rent lease</td>\n",
       "      <td>cum d.n. Andheri 7</td>\n",
       "      <td>25/07/2023</td>\n",
       "      <td>1) Bhavin Sheth, Authorized Trustee on behalf ...</td>\n",
       "      <td>1) Hanmant Dhanvare, Executive Engineer, Mahar...</td>\n",
       "      <td>1) Other information: Piece or parcel of land ...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "      <td>Rent lease__$__cum d.n. Andheri 7__$__1) Bhavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4286</td>\n",
       "      <td>Development Agreement</td>\n",
       "      <td>Co. Andheri 7</td>\n",
       "      <td>10/03/2023</td>\n",
       "      <td>1) Ajit Singh Kartar Singh Chandok</td>\n",
       "      <td>1) Paresh Ranchod Patel, Partner, Navish Realt...</td>\n",
       "      <td>1) Other information: Land and construction, p...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "      <td>Development Agreement__$__Co. Andheri 7__$__1)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2449</td>\n",
       "      <td>Sale Deed</td>\n",
       "      <td>Co Dn. Andheri 7</td>\n",
       "      <td>09/02/2023</td>\n",
       "      <td>1) Nupur Anil Kalke Mukhtyar Shaila Anil Kalke...</td>\n",
       "      <td>1) Harsh Satpal Malhotra</td>\n",
       "      <td>1) Other Information: House No: 701, Floor No:...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "      <td>Sale Deed__$__Co Dn. Andheri 7__$__1) Nupur An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8691</td>\n",
       "      <td>66-Notice of Lis Pendency</td>\n",
       "      <td>cum. Andheri 7</td>\n",
       "      <td>22/05/2023</td>\n",
       "      <td>1) Pradeep Soni</td>\n",
       "      <td></td>\n",
       "      <td>1) Other Information: City Civil Court at Dind...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "      <td>66-Notice of Lis Pendency__$__cum. Andheri 7__...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3551</td>\n",
       "      <td>65-Mistake Correction Letter</td>\n",
       "      <td>with D.N. Andheri 7</td>\n",
       "      <td>27/02/2023</td>\n",
       "      <td>1) Gunjan Yogeet Kapoor\\n2) Tushar Subhash Oberoi</td>\n",
       "      <td>1) Shama Subhash Oberoi</td>\n",
       "      <td>1) House No: 201, Mala No: 2, Building Boat: O...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "      <td>65-Mistake Correction Letter__$__with D.N. And...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>1204</td>\n",
       "      <td>1205</td>\n",
       "      <td>3173</td>\n",
       "      <td>Live Ad Licenses</td>\n",
       "      <td>Co D.N. ANDHERI 7</td>\n",
       "      <td>20/02/2023</td>\n",
       "      <td>1) COELHO NOALA</td>\n",
       "      <td>1) MUHAMMAD KUNHI K C</td>\n",
       "      <td>1) FLAT NO:35/A, MLA NO:1ST FLOOR, BUILDING NO...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "      <td>Live Ad Licenses__$__Co D.N. ANDHERI 7__$__1) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>1205</td>\n",
       "      <td>1206</td>\n",
       "      <td>2531</td>\n",
       "      <td>Agreement</td>\n",
       "      <td>Co.D. Andheri 7</td>\n",
       "      <td>13/02/2023</td>\n",
       "      <td>1) Chandresh Mehta, Director of Keystone Realt...</td>\n",
       "      <td>1) Rashmi Satish Kewalramani.\\n2) Varun Satish...</td>\n",
       "      <td>1) Other Information: Real Estate Project-\\\"Ru...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "      <td>Agreement__$__Co.D. Andheri 7__$__1) Chandresh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>1206</td>\n",
       "      <td>1207</td>\n",
       "      <td>2863</td>\n",
       "      <td>Live Ad Licenses</td>\n",
       "      <td>Co D.N. Andheri 7</td>\n",
       "      <td>15/02/2023</td>\n",
       "      <td>1) Faisal Abdul Rahim Ghori</td>\n",
       "      <td>1) Jeram Chamadia (HUF) Laxman</td>\n",
       "      <td>1) Flat No:Garage No.1, Floor No:Ground, Build...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "      <td>Live Ad Licenses__$__Co D.N. Andheri 7__$__1) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>1207</td>\n",
       "      <td>1208</td>\n",
       "      <td>2506</td>\n",
       "      <td>Release Deed</td>\n",
       "      <td>Co Dn. Andheri 7</td>\n",
       "      <td>10/02/2023</td>\n",
       "      <td>1) Monisha Rahim Harjee alias Monisha B.Charan...</td>\n",
       "      <td>1) Karim B. Charaniya</td>\n",
       "      <td>1) House No: 1201, Floor No: Barawa Majla, Bui...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "      <td>Release Deed__$__Co Dn. Andheri 7__$__1) Monis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>1208</td>\n",
       "      <td>1209</td>\n",
       "      <td>6172</td>\n",
       "      <td>Deed of Transfer</td>\n",
       "      <td>Co.D. Andheri 7</td>\n",
       "      <td>11/04/2023</td>\n",
       "      <td>1) Mr. Ashish Nalwaya, Partner, on behalf of S...</td>\n",
       "      <td>1) Mr. Pratyush Bharatiya, Partner, on behalf ...</td>\n",
       "      <td>1) Other Information: Total area of ​​land and...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "      <td>Deed of Transfer__$__Co.D. Andheri 7__$__1) Mr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1209 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index Sl no. Diarrhea no.                 diarrhea type  \\\n",
       "0        0      1        13217                    Rent lease   \n",
       "1        1      2         4286         Development Agreement   \n",
       "2        2      3         2449                     Sale Deed   \n",
       "3        3      4         8691     66-Notice of Lis Pendency   \n",
       "4        4      5         3551  65-Mistake Correction Letter   \n",
       "...    ...    ...          ...                           ...   \n",
       "1204  1204   1205         3173              Live Ad Licenses   \n",
       "1205  1205   1206         2531                     Agreement   \n",
       "1206  1206   1207         2863              Live Ad Licenses   \n",
       "1207  1207   1208         2506                  Release Deed   \n",
       "1208  1208   1209         6172              Deed of Transfer   \n",
       "\n",
       "     Du. Prohibit. Office        Year  \\\n",
       "0      cum d.n. Andheri 7  25/07/2023   \n",
       "1           Co. Andheri 7  10/03/2023   \n",
       "2        Co Dn. Andheri 7  09/02/2023   \n",
       "3          cum. Andheri 7  22/05/2023   \n",
       "4     with D.N. Andheri 7  27/02/2023   \n",
       "...                   ...         ...   \n",
       "1204    Co D.N. ANDHERI 7  20/02/2023   \n",
       "1205      Co.D. Andheri 7  13/02/2023   \n",
       "1206    Co D.N. Andheri 7  15/02/2023   \n",
       "1207     Co Dn. Andheri 7  10/02/2023   \n",
       "1208      Co.D. Andheri 7  11/04/2023   \n",
       "\n",
       "                                             buyer_name  \\\n",
       "0     1) Bhavin Sheth, Authorized Trustee on behalf ...   \n",
       "1                    1) Ajit Singh Kartar Singh Chandok   \n",
       "2     1) Nupur Anil Kalke Mukhtyar Shaila Anil Kalke...   \n",
       "3                                       1) Pradeep Soni   \n",
       "4     1) Gunjan Yogeet Kapoor\\n2) Tushar Subhash Oberoi   \n",
       "...                                                 ...   \n",
       "1204                                    1) COELHO NOALA   \n",
       "1205  1) Chandresh Mehta, Director of Keystone Realt...   \n",
       "1206                        1) Faisal Abdul Rahim Ghori   \n",
       "1207  1) Monisha Rahim Harjee alias Monisha B.Charan...   \n",
       "1208  1) Mr. Ashish Nalwaya, Partner, on behalf of S...   \n",
       "\n",
       "                                            seller_name  \\\n",
       "0     1) Hanmant Dhanvare, Executive Engineer, Mahar...   \n",
       "1     1) Paresh Ranchod Patel, Partner, Navish Realt...   \n",
       "2                              1) Harsh Satpal Malhotra   \n",
       "3                                                         \n",
       "4                               1) Shama Subhash Oberoi   \n",
       "...                                                 ...   \n",
       "1204                              1) MUHAMMAD KUNHI K C   \n",
       "1205  1) Rashmi Satish Kewalramani.\\n2) Varun Satish...   \n",
       "1206                     1) Jeram Chamadia (HUF) Laxman   \n",
       "1207                              1) Karim B. Charaniya   \n",
       "1208  1) Mr. Pratyush Bharatiya, Partner, on behalf ...   \n",
       "\n",
       "                                      Other information  \\\n",
       "0     1) Other information: Piece or parcel of land ...   \n",
       "1     1) Other information: Land and construction, p...   \n",
       "2     1) Other Information: House No: 701, Floor No:...   \n",
       "3     1) Other Information: City Civil Court at Dind...   \n",
       "4     1) House No: 201, Mala No: 2, Building Boat: O...   \n",
       "...                                                 ...   \n",
       "1204  1) FLAT NO:35/A, MLA NO:1ST FLOOR, BUILDING NO...   \n",
       "1205  1) Other Information: Real Estate Project-\\\"Ru...   \n",
       "1206  1) Flat No:Garage No.1, Floor No:Ground, Build...   \n",
       "1207  1) House No: 1201, Floor No: Barawa Majla, Bui...   \n",
       "1208  1) Other Information: Total area of ​​land and...   \n",
       "\n",
       "                                             List no. 2  \\\n",
       "0     https://pay2igr.igrmaharashtra.gov.in/eDisplay...   \n",
       "1     https://pay2igr.igrmaharashtra.gov.in/eDisplay...   \n",
       "2     https://pay2igr.igrmaharashtra.gov.in/eDisplay...   \n",
       "3     https://pay2igr.igrmaharashtra.gov.in/eDisplay...   \n",
       "4     https://pay2igr.igrmaharashtra.gov.in/eDisplay...   \n",
       "...                                                 ...   \n",
       "1204  https://pay2igr.igrmaharashtra.gov.in/eDisplay...   \n",
       "1205  https://pay2igr.igrmaharashtra.gov.in/eDisplay...   \n",
       "1206  https://pay2igr.igrmaharashtra.gov.in/eDisplay...   \n",
       "1207  https://pay2igr.igrmaharashtra.gov.in/eDisplay...   \n",
       "1208  https://pay2igr.igrmaharashtra.gov.in/eDisplay...   \n",
       "\n",
       "                                           to_translate  \n",
       "0     Rent lease__$__cum d.n. Andheri 7__$__1) Bhavi...  \n",
       "1     Development Agreement__$__Co. Andheri 7__$__1)...  \n",
       "2     Sale Deed__$__Co Dn. Andheri 7__$__1) Nupur An...  \n",
       "3     66-Notice of Lis Pendency__$__cum. Andheri 7__...  \n",
       "4     65-Mistake Correction Letter__$__with D.N. And...  \n",
       "...                                                 ...  \n",
       "1204  Live Ad Licenses__$__Co D.N. ANDHERI 7__$__1) ...  \n",
       "1205  Agreement__$__Co.D. Andheri 7__$__1) Chandresh...  \n",
       "1206  Live Ad Licenses__$__Co D.N. Andheri 7__$__1) ...  \n",
       "1207  Release Deed__$__Co Dn. Andheri 7__$__1) Monis...  \n",
       "1208  Deed of Transfer__$__Co.D. Andheri 7__$__1) Mr...  \n",
       "\n",
       "[1209 rows x 11 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_translated_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del os.environ['PYSPARK_PYTHON'] \n",
    "# del os.environ['PYSPARK_DRIVER_PYTHON'] \n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable \n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "del spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://FoxFace:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24342b385b0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach Via Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# spark = SparkSession.builder \\\n",
    "#         .config('spark.executor.instances', 4) \\\n",
    "#         .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "#         .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "#         .config(\"spark.dynamicAllocation.minExecutors\",\"1\") \\\n",
    "#         .config(\"spark.dynamicAllocation.maxExecutors\",\"5\") \\\n",
    "#         .config(\"spark.executor.cores\",6) \\\n",
    "#         .config(\"spark.executor.memory\",\"2g\") \\\n",
    "#         .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "# #spark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.executor.cores\", 4)\n",
    "# spark.conf.set(\"spark.dynamicAllocation.minExecutors\",\"1\")\n",
    "# spark.conf.set(\"spark.dynamicAllocation.maxExecutors\",\"5\")\n",
    "# conf = spark.sparkContext.getConf()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Data from SQL directly\n",
    "host = '127.0.0.1'\n",
    "port = '5432'\n",
    "database = 'propReturns'\n",
    "username = 'postgres'\n",
    "password = 'Sunrise12345'\n",
    "url = f\"jdbc:postgresql://{host}:{port}/{database}\"\n",
    "table_name = 'record_details_raw'\n",
    "\n",
    "engine = create_engine(f'postgresql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "spark_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password).load()\n",
    "\n",
    "records_raw = pd.read_sql('record_details_raw',engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for col in records_raw.columns:\n",
    "    new_names.append(translator(col,source='auto'))\n",
    "\n",
    "new_names = [x.replace('.','') for x in new_names]\n",
    "spark_df = spark_df.toDF(*new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|index|Sl no|Diarrhea no|       diarrhea type|Du Prohibit Office|      Year|          Will write|     Will write down|   Other information|           List no 2|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|    1|      13217|           भाडेपट्टा|सह दु.नि. अंधेरी 7|25/07/2023|1) श्रीमती चंद्रक...|1) महाराष्ट़ हाऊस...|1) इतर माहिती: पि...|https://pay2igr.i...|\n",
      "|    1|    2|       4286|       विकसनकरारनामा|सह दु.नि. अंधेरी 7|10/03/2023|1) अजित सिंह करता...|1) नाविश रियल्टी ...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|    2|    3|       2449|             सेल डीड|सह दु.नि. अंधेरी 7|09/02/2023|1) नूपुर अनिल कळक...|1) हर्ष सतपाल मल्...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|    3|    4|       8691|66-नोटीस ऑफ़ लिस प...|सह दु.नि. अंधेरी 7|22/05/2023|      1) प्रदीप सोनी|                    |1) इतर माहिती: सि...|https://pay2igr.i...|\n",
      "|    4|    5|       3551|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|27/02/2023|1) गुंजन योगीत कप...|1) शामा सुभाष ओबेराय|1) सदनिका नं: 201...|https://pay2igr.i...|\n",
      "|    5|    6|       8043|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|19/05/2023|1) राधिका खंडेलवा...|1) व्हीबीएचडीसी ब...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   12|   13|       7415|       विकसनकरारनामा|सह दु.नि. अंधेरी 7|02/05/2023|1) सोसायटी मेंबर ...|1) इंस्पायरा बिल्...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|    6|    7|       5454|           भाडेपट्टा|सह दु.नि. अंधेरी 7|29/03/2023|1) नियोजन को ऑपरे...|1) मुंबई हाऊसिंग ...|1) सदनिका नं: प्ल...|https://pay2igr.i...|\n",
      "|    7|    8|       7732|    अभिहस्तांतरणपत्र|सह दु.नि. अंधेरी 7|04/05/2023|1) मे एकता सुप्री...|1) युडोरा कॉ ऑप ह...|1) इतर माहिती: इम...|https://pay2igr.i...|\n",
      "|    8|    9|       5457|           भाडेपट्टा|सह दु.नि. अंधेरी 7|29/03/2023|1) साई दत्त प्रसा...|1) मुंबई हाऊसिंग ...|1) सदनिका नं: प्ल...|https://pay2igr.i...|\n",
      "|    9|   10|       8896|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|24/05/2023|1) . . गोपाल नारा...|1) . . श्रीधर गोप...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|   10|   11|       2389|       प्रतिज्ञापत्र|सह दु.नि. अंधेरी 7|08/02/2023|1) गांघी नगर गणेश...|                    |1) इतर माहिती: re...|https://pay2igr.i...|\n",
      "|   11|   12|       8895|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|24/05/2023|1) . . गोपाल नारा...|1) . . नेहा गोपाल...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|   13|   14|        212|             सेल डीड|सह दु.नि. अंधेरी 7|04/01/2023|1) पुष्पा बी रहेज...|1) कलश दिलीप सुरा...|1) सदनिका नं: 501...|https://pay2igr.i...|\n",
      "|   14|   15|       7019|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|21/04/2023|1) मिशेल देसाई पू...|1) किऑर्बिट रिअल्...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   15|   16|       5868|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|12/04/2023|1) बांद्रा विनय क...|1) डायनास्टी इन्फ...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   16|   17|       5450|              लीजडीड|सह दु.नि. अंधेरी 7|29/03/2023|1) दि एम आय जी को...|1) (मालक ) महाराष...|1) इतर माहिती: टी...|https://pay2igr.i...|\n",
      "|   17|   18|       3753|             सेल डीड|सह दु.नि. अंधेरी 7|01/03/2023|1) राहुल विनायक ड...|1) रमेशकुमार मानक...|1) सदनिका नं: 801...|https://pay2igr.i...|\n",
      "|   18|   19|       8517|66-नोटीस ऑफ़ लिस प...|सह दु.नि. अंधेरी 7|18/05/2023|1) व्हिज एंटरप्रा...|                    |1) सदनिका नं: 602...|https://pay2igr.i...|\n",
      "|   49|   50|        496| लिव्ह अॅड लायसन्सेस|सह दु.नि. अंधेरी 7|10/01/2023|1) लीना किरण निसा...|1) रैलोगिक कंट्रो...|1) फ़्लॅट नं:एल जी...|https://pay2igr.i...|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- Sl no: string (nullable = true)\n",
      " |-- Diarrhea no: string (nullable = true)\n",
      " |-- diarrhea type: string (nullable = true)\n",
      " |-- Du Prohibit Office: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Will write: string (nullable = true)\n",
      " |-- Will write down: string (nullable = true)\n",
      " |-- Other information: string (nullable = true)\n",
      " |-- List no 2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'quoted_name' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mselect(concat(\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'quoted_name' object is not callable"
     ]
    }
   ],
   "source": [
    "spark_df.select(concat(col(\"index\"), lit(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'quoted_name' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# spark_df.withColumn('to_translate',concat(col('diarrhea type'),col('Other information')))\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_translate\u001b[39m\u001b[38;5;124m\"\u001b[39m, concat(\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiarrhea type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOther information\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'quoted_name' object is not callable"
     ]
    }
   ],
   "source": [
    "# spark_df.withColumn('to_translate',concat(col('diarrhea type'),col('Other information')))\n",
    "spark_df.withColumn(\"to_translate\", concat(col(\"indexs\"), col(\"Other information\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'quoted_name' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_translate\u001b[39m\u001b[38;5;124m\"\u001b[39m,concat(\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiarrhea type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,lit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__$__\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      2\u001b[0m                                           col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDu Prohibit Office\u001b[39m\u001b[38;5;124m'\u001b[39m),lit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__$__\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      3\u001b[0m                                           col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDu Prohibit Office\u001b[39m\u001b[38;5;124m'\u001b[39m),lit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__$__\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      4\u001b[0m                                           col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDu Prohibit Office\u001b[39m\u001b[38;5;124m'\u001b[39m),lit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__$__\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      5\u001b[0m                                           col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDu Prohibit Office\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'quoted_name' object is not callable"
     ]
    }
   ],
   "source": [
    "spark_df.withColumn(\"to_translate\",concat(col(\"diarrhea type\"),lit('__$__'),\n",
    "                                          col('Du Prohibit Office'),lit('__$__'),\n",
    "                                          col('Du Prohibit Office'),lit('__$__'),\n",
    "                                          col('Du Prohibit Office'),lit('__$__'),\n",
    "                                          col('Du Prohibit Office')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('diarrhea type',split(spark_df['to_translate'],'__$__')[0])\\\n",
    "        .withColumn('Du Prohibit Office',split(spark_df['to_translate'],'__$__')[1])\\\n",
    "        .withColumn('Will write',split(spark_df['to_translate'],'__$__')[2])\\\n",
    "        .withColumn('Will write down',split(spark_df['to_translate'],'__$__')[3])\\\n",
    "        .withColumn('Other information',split(spark_df['to_translate'],'__$__')[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+--------------------+------------------+----------+----------+---------------+-----------------+--------------------+--------------------+\n",
      "|index|Sl no|Diarrhea no|       diarrhea type|Du Prohibit Office|      Year|Will write|Will write down|Other information|           List no 2|        to_translate|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+----------+---------------+-----------------+--------------------+--------------------+\n",
      "|    0|    1|      13217|Rent lease__$__cu...|              NULL|25/07/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Rent lease__$__cu...|\n",
      "|    1|    2|       4286|Development Agree...|              NULL|10/03/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Development Agree...|\n",
      "|    2|    3|       2449|Sale Deed__$__Co ...|              NULL|09/02/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Sale Deed__$__Co ...|\n",
      "|    3|    4|       8691|66-Notice of Lis ...|              NULL|22/05/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|66-Notice of Lis ...|\n",
      "|    4|    5|       3551|65-Mistake Correc...|              NULL|27/02/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|65-Mistake Correc...|\n",
      "|    5|    6|       8043|Development Agree...|              NULL|19/05/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Development Agree...|\n",
      "|   12|   13|       7415|Development Agree...|              NULL|02/05/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Development Agree...|\n",
      "|    6|    7|       5454|Rent lease__$__cu...|              NULL|29/03/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Rent lease__$__cu...|\n",
      "|    7|    8|       7732|Deed of Transfer_...|              NULL|04/05/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Deed of Transfer_...|\n",
      "|    8|    9|       5457|Rent lease__$__cu...|              NULL|29/03/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Rent lease__$__cu...|\n",
      "|    9|   10|       8896|65-Mistake Correc...|              NULL|24/05/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|65-Mistake Correc...|\n",
      "|   10|   11|       2389|Affidavit__$__cum...|              NULL|08/02/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Affidavit__$__cum...|\n",
      "|   11|   12|       8895|65-Mistake Correc...|              NULL|24/05/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|65-Mistake Correc...|\n",
      "|   13|   14|        212|Sale Deed__$__Co ...|              NULL|04/01/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Sale Deed__$__Co ...|\n",
      "|   14|   15|       7019|Development Agree...|              NULL|21/04/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Development Agree...|\n",
      "|   15|   16|       5868|Development Agree...|              NULL|12/04/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Development Agree...|\n",
      "|   16|   17|       5450|Leasedeed__$__co ...|              NULL|29/03/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Leasedeed__$__co ...|\n",
      "|   17|   18|       3753|Sale Deed__$__Co ...|              NULL|01/03/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Sale Deed__$__Co ...|\n",
      "|   18|   19|       8517|66-Notice of Lis ...|              NULL|18/05/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|66-Notice of Lis ...|\n",
      "|   49|   50|        496|Live Ad Licenses_...|              NULL|10/01/2023|      NULL|           NULL|             NULL|https://pay2igr.i...|Live Ad Licenses_...|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+----------+---------------+-----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_translate = [\n",
    "    'diarrhea type',\n",
    "    'Du Prohibit Office',\n",
    "    'Will write', \n",
    "    'Will write down', \n",
    "    'Other information'\n",
    "    ]\n",
    "\n",
    "for column in cols_to_translate:\n",
    "    spark_df = spark_df.withColumn(column, translate_udf(spark_df[column]))\n",
    "\n",
    "spark_df = spark_df.withColumnRenamed(existing='Will write',new='buyer_name')\n",
    "spark_df = spark_df.withColumnRenamed(existing='Will write down',new='seller_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|index|Sl no|Diarrhea no|       diarrhea type|Du Prohibit Office|      Year|          buyer_name|         seller_name|   Other information|           List no 2|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|    1|       4286|development agree...|Co. D.N. Andheri 7|10/03/2023|1) Ajit Singh Kar...|1) Paresh Ranchho...|1) Other informat...|https://pay2igr.i...|\n",
      "|    1|    2|      13217|               lease|Co. D.N. Andheri 7|25/07/2023|1) Bhavin Sheth, ...|1) Hanmant Dhanva...|1) Other informat...|https://pay2igr.i...|\n",
      "|    2|    3|       2449|           sale deed|Co. D.N. Andheri 7|09/02/2023|1) Nupur Anil Kal...|1) Harsh Satpal M...|1) Other Informat...|https://pay2igr.i...|\n",
      "|    3|    4|       8691|66-Notice of list...|Co. D.N. Andheri 7|22/05/2023|     1) Pradeep Soni|                null|1) Other Informat...|https://pay2igr.i...|\n",
      "|    4|    5|       3551|65-correction letter|Co. D.N. Andheri 7|27/02/2023|1) Gunjan Yogeet ...|1) Shama Subhash ...|1) House No: 201,...|https://pay2igr.i...|\n",
      "|    5|    6|       8043|development agree...|Co. D.N. Andheri 7|19/05/2023|1) Radhika Khande...|1) Suraj Kumar Du...|1) Other informat...|https://pay2igr.i...|\n",
      "|   12|   13|       7415|development agree...|Co. D.N. Andheri 7|02/05/2023|1) सोसायटी मेंबर ...|1) Inspira Buildc...|1) Other informat...|https://pay2igr.i...|\n",
      "|    6|    7|       5454|               lease|Co. D.N. Andheri 7|29/03/2023|1) Planning was t...|1) T R Chatupule,...|1) House No.: Plo...|https://pay2igr.i...|\n",
      "|    7|    8|       5457|               lease|Co. D.N. Andheri 7|29/03/2023|1) Sai Dutt Prasa...|1) Ashok Kajne, E...|1) House No.: Plo...|https://pay2igr.i...|\n",
      "|    8|    9|       7732|    deed of transfer|Co. D.N. Andheri 7|04/05/2023|1) Chief Executiv...|1) Khazindar Rama...|1) Other Details:...|https://pay2igr.i...|\n",
      "|    9|   10|       8895|65-correction letter|Co. D.N. Andheri 7|24/05/2023|1) . , Gopal Nara...|1) . , Neha Gopal...|1) Other informat...|https://pay2igr.i...|\n",
      "|   10|   11|       8896|65-correction letter|Co. D.N. Andheri 7|24/05/2023|1) . , Gopal Nara...|1) . , Sridhar Go...|1) Other informat...|https://pay2igr.i...|\n",
      "|   11|   12|       2389|     promissory note|Co. D.N. Andheri 7|08/02/2023|1) Gandhi Nagar G...|                null|1) Other informat...|https://pay2igr.i...|\n",
      "|   13|   14|        212|           sale deed|Co. D.N. Andheri 7|04/01/2023|1) Mukhtyar Rishi...|1) Kalash Dilip S...|1) House No: 501,...|https://pay2igr.i...|\n",
      "|   14|   15|       7019|development agree...|Co. D.N. Andheri 7|21/04/2023|1) Michel Desai P...|1) Chief Geeta Mo...|1) Other informat...|https://pay2igr.i...|\n",
      "|   15|   16|       5868|development agree...|Co. D.N. Andheri 7|12/04/2023|1) Chairman Jaywa...|1) Krunal Sheth, ...|1) Other informat...|https://pay2igr.i...|\n",
      "|   16|   17|       5450|           leasedeed|Co. D.N. Andheri 7|29/03/2023|1) Vijay Fatarfek...|1) (Owner) Neelim...|1) Other Informat...|https://pay2igr.i...|\n",
      "|   17|   18|       3753|           sale deed|Co. D.N. Andheri 7|01/03/2023|1) Rahul Vinayak ...|1) Rameshkumar Ma...|1) House No: 801 ...|https://pay2igr.i...|\n",
      "|   18|   19|       8517|66-Notice of list...|Co. D.N. Andheri 7|18/05/2023|1) Whiz Enterpris...|                null|1) House No: 602,...|https://pay2igr.i...|\n",
      "|   33|   34|       3172|    live ad licenses|Co. D.N. Andheri 7|20/02/2023|      1) Bimla Devi.|1) Puriben Bachch...|1) Flat No: Plot ...|https://pay2igr.i...|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o144.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:570)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:82)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:570)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m table_name_sink \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord_details_translated\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name_sink\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o144.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:570)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:82)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:570)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "table_name_sink = 'record_details_translated'\n",
    "\n",
    "spark_df.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", table_name_sink) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_translated = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Sl no</th>\n",
       "      <th>Diarrhea no</th>\n",
       "      <th>diarrhea type</th>\n",
       "      <th>Du Prohibit Office</th>\n",
       "      <th>Year</th>\n",
       "      <th>buyer_name</th>\n",
       "      <th>seller_name</th>\n",
       "      <th>Other information</th>\n",
       "      <th>List no 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13217</td>\n",
       "      <td>lease</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>25/07/2023</td>\n",
       "      <td>1) Bhavin Sheth, Authorized Trustee on behalf ...</td>\n",
       "      <td>1) Hanmant Dhanvare, Executive Engineer, Mahar...</td>\n",
       "      <td>1) Other information: Piece or parcel of land ...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4286</td>\n",
       "      <td>development agreement</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>10/03/2023</td>\n",
       "      <td>1) Ajit Singh Kartar Singh Chandok</td>\n",
       "      <td>1) Paresh Ranchhod Patel, Partner, Navish Real...</td>\n",
       "      <td>1) Other information: Land and construction, p...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2449</td>\n",
       "      <td>sale deed</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>09/02/2023</td>\n",
       "      <td>1) Nupur Anil Kalke by Mukhtyar Shaila Anil Ka...</td>\n",
       "      <td>1) Harsh Satpal Malhotra</td>\n",
       "      <td>1) Other Information: House No: 701, Floor No:...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8691</td>\n",
       "      <td>66-Notice of list pendency</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>22/05/2023</td>\n",
       "      <td>1) Pradeep Soni</td>\n",
       "      <td>null</td>\n",
       "      <td>1) Other Information: City Civil Court at Dind...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3551</td>\n",
       "      <td>65-correction letter</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>27/02/2023</td>\n",
       "      <td>1) Gunjan Yogeet Kapoor\\n2) Tushar Subhash Oberoi</td>\n",
       "      <td>1) Shama Subhash Oberoi</td>\n",
       "      <td>1) House No: 201, Mala No: 2, Building Boat: O...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>1204</td>\n",
       "      <td>1205</td>\n",
       "      <td>3173</td>\n",
       "      <td>live ad licenses</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>20/02/2023</td>\n",
       "      <td>1) Coelho Noala</td>\n",
       "      <td>1) Muhammad Kunhi KC</td>\n",
       "      <td>1) Flat No:35/A, Floor No:1ST FLOOR, Building ...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>1205</td>\n",
       "      <td>1206</td>\n",
       "      <td>2531</td>\n",
       "      <td>agreement</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>13/02/2023</td>\n",
       "      <td>1) Chief Sandeep Gawde on behalf of Chandresh ...</td>\n",
       "      <td>1) Rashmi Satish Kewalramani\\n2) Varun Satish ...</td>\n",
       "      <td>1) Other Information: Real Estate Project-\\\"Ru...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>1206</td>\n",
       "      <td>1207</td>\n",
       "      <td>2863</td>\n",
       "      <td>live ad licenses</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>15/02/2023</td>\n",
       "      <td>1) Faisal Abdul Rahim Ghori</td>\n",
       "      <td>1) Jeram Chamadia (HUF) Laxman</td>\n",
       "      <td>1) Flat No.: Garage No. 1, Floor No.: Ground, ...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>1207</td>\n",
       "      <td>1208</td>\n",
       "      <td>2506</td>\n",
       "      <td>release deed</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>10/02/2023</td>\n",
       "      <td>1) Monisha Rahim Harji alias Monisha B.Charani...</td>\n",
       "      <td>1) Karim B. pasture</td>\n",
       "      <td>1) House No: 1201, Floor No: Barawa Majla, Bui...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>1208</td>\n",
       "      <td>1209</td>\n",
       "      <td>6172</td>\n",
       "      <td>deed of transfer</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>11/04/2023</td>\n",
       "      <td>1) Mr. Ashish Nalwaya, Partner, Saffron Ambit ...</td>\n",
       "      <td>1) Mr. Pratyush Bharatiya, Partner, Saffron In...</td>\n",
       "      <td>1) Other information: Total area of ​​land and...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1209 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index Sl no Diarrhea no               diarrhea type  Du Prohibit Office  \\\n",
       "0         0     1       13217                       lease  Co. D.N. Andheri 7   \n",
       "1         1     2        4286       development agreement  Co. D.N. Andheri 7   \n",
       "2         2     3        2449                   sale deed  Co. D.N. Andheri 7   \n",
       "3         3     4        8691  66-Notice of list pendency  Co. D.N. Andheri 7   \n",
       "4         4     5        3551        65-correction letter  Co. D.N. Andheri 7   \n",
       "...     ...   ...         ...                         ...                 ...   \n",
       "1204   1204  1205        3173            live ad licenses  Co. D.N. Andheri 7   \n",
       "1205   1205  1206        2531                   agreement  Co. D.N. Andheri 7   \n",
       "1206   1206  1207        2863            live ad licenses  Co. D.N. Andheri 7   \n",
       "1207   1207  1208        2506                release deed  Co. D.N. Andheri 7   \n",
       "1208   1208  1209        6172            deed of transfer  Co. D.N. Andheri 7   \n",
       "\n",
       "            Year                                         buyer_name  \\\n",
       "0     25/07/2023  1) Bhavin Sheth, Authorized Trustee on behalf ...   \n",
       "1     10/03/2023                 1) Ajit Singh Kartar Singh Chandok   \n",
       "2     09/02/2023  1) Nupur Anil Kalke by Mukhtyar Shaila Anil Ka...   \n",
       "3     22/05/2023                                    1) Pradeep Soni   \n",
       "4     27/02/2023  1) Gunjan Yogeet Kapoor\\n2) Tushar Subhash Oberoi   \n",
       "...          ...                                                ...   \n",
       "1204  20/02/2023                                    1) Coelho Noala   \n",
       "1205  13/02/2023  1) Chief Sandeep Gawde on behalf of Chandresh ...   \n",
       "1206  15/02/2023                        1) Faisal Abdul Rahim Ghori   \n",
       "1207  10/02/2023  1) Monisha Rahim Harji alias Monisha B.Charani...   \n",
       "1208  11/04/2023  1) Mr. Ashish Nalwaya, Partner, Saffron Ambit ...   \n",
       "\n",
       "                                            seller_name  \\\n",
       "0     1) Hanmant Dhanvare, Executive Engineer, Mahar...   \n",
       "1     1) Paresh Ranchhod Patel, Partner, Navish Real...   \n",
       "2                              1) Harsh Satpal Malhotra   \n",
       "3                                                  null   \n",
       "4                               1) Shama Subhash Oberoi   \n",
       "...                                                 ...   \n",
       "1204                               1) Muhammad Kunhi KC   \n",
       "1205  1) Rashmi Satish Kewalramani\\n2) Varun Satish ...   \n",
       "1206                     1) Jeram Chamadia (HUF) Laxman   \n",
       "1207                                1) Karim B. pasture   \n",
       "1208  1) Mr. Pratyush Bharatiya, Partner, Saffron In...   \n",
       "\n",
       "                                      Other information  \\\n",
       "0     1) Other information: Piece or parcel of land ...   \n",
       "1     1) Other information: Land and construction, p...   \n",
       "2     1) Other Information: House No: 701, Floor No:...   \n",
       "3     1) Other Information: City Civil Court at Dind...   \n",
       "4     1) House No: 201, Mala No: 2, Building Boat: O...   \n",
       "...                                                 ...   \n",
       "1204  1) Flat No:35/A, Floor No:1ST FLOOR, Building ...   \n",
       "1205  1) Other Information: Real Estate Project-\\\"Ru...   \n",
       "1206  1) Flat No.: Garage No. 1, Floor No.: Ground, ...   \n",
       "1207  1) House No: 1201, Floor No: Barawa Majla, Bui...   \n",
       "1208  1) Other information: Total area of ​​land and...   \n",
       "\n",
       "                                              List no 2  \n",
       "0     https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "1     https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "2     https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "3     https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "4     https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "...                                                 ...  \n",
       "1204  https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "1205  https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "1206  https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "1207  https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "1208  https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "\n",
       "[1209 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# records_translated = spark_df.toPandas()\n",
    "table_name = 'record_details_translated'\n",
    "records_translated.to_sql(table_name, engine, if_exists='replace', index=True)\n",
    "print(f\"{len(records_translate)} DataFrame has been inserted into the '{table_name}' table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Data from SQL directly\n",
    "host = '127.0.0.1'\n",
    "port = '5432'\n",
    "database = 'propReturns'\n",
    "username = 'postgres'\n",
    "password = 'Sunrise12345'\n",
    "url = f\"jdbc:postgresql://{host}:{port}/{database}\"\n",
    "table_name = 'record_details_translated_v2'\n",
    "\n",
    "engine = create_engine(f'postgresql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# spark_df = spark.read.format(\"jdbc\") \\\n",
    "#         .option(\"url\", url) \\\n",
    "#         .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "#         .option(\"dbtable\", table_name) \\\n",
    "#         .option(\"user\", username) \\\n",
    "#         .option(\"password\", password).load()\n",
    "\n",
    "records_translated = pd.read_sql('record_details_translated_v2',engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Sl no', 'Diarrhea no', 'diarrhea type', 'Du Prohibit Office',\n",
       "       'Year', 'buyer_name', 'seller_name', 'Other information', 'List no 2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_translated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "# This restores the same behavior as before.\n",
    "context = ssl._create_unverified_context()\n",
    "# urllib.urlopen(\"https://no-valid-cert\", context=context)\n",
    "pdf_path = \"https://pay2igr.igrmaharashtra.gov.in/eDisplay/propertydetails/indexii/Mw%3D%3D/MDUxNDA1MzY5OTAwMDAwNDI4NjIwMjNJUw%3D%3D\"\n",
    "def download_file(download_url, filename):\n",
    "    response = urllib.request.urlopen(download_url,context=context)    \n",
    "    file = open(filename + \".pdf\", 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()\n",
    " \n",
    "download_file(download_url=pdf_path, filename=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [0,100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:39,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [100,200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:45,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [200,300]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:34,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [300,400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:35,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [400,500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:32,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [500,600]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:34,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [600,700]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:30,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [700,800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:44,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [800,900]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:38,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [900,1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:02,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [1000,1100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:39,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [1100,1200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:34,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [1200,1300]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:08,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "#Approach Via Pandas --> Just downloading the files\n",
    "batch_size = 100\n",
    "start = 0\n",
    "end = len(records_translated)\n",
    "# end = 5\n",
    "os.makedirs(\"pdfs\", exist_ok = True)\n",
    "for batch in range(start,end,batch_size):\n",
    "    print(f'Batch -> [{batch},{batch+batch_size}]')\n",
    "    temp_df = records_translated.iloc[batch:batch+batch_size,:].copy()\n",
    "    for index, row in tqdm(temp_df.iterrows()):\n",
    "        download_file(download_url=row['List no 2'], filename=f\"pdfs/{row['Sl no']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curves_to_edges(cs):\n",
    "    edges = []\n",
    "    for c in cs:\n",
    "        edges += pdfplumber.utils.rect_to_edges(c)\n",
    "    return edges\n",
    "\n",
    "# Get the bounding boxes of the tables on the page.\n",
    "def not_within_bboxes(obj):\n",
    "    \"\"\"Check if the object is in any of the table's bbox.\"\"\"\n",
    "    def obj_in_bbox(_bbox):\n",
    "        \"\"\"See https://github.com/jsvine/pdfplumber/blob/stable/pdfplumber/table.py#L404\"\"\"\n",
    "        v_mid = (obj[\"top\"] + obj[\"bottom\"]) / 2\n",
    "        h_mid = (obj[\"x0\"] + obj[\"x1\"]) / 2\n",
    "        x0, top, x1, bottom = _bbox\n",
    "        return (h_mid >= x0) and (h_mid < x1) and (v_mid >= top) and (v_mid < bottom)\n",
    "    return not any(obj_in_bbox(__bbox) for __bbox in bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'C:\\Users\\ayush\\OneDrive\\Desktop\\Projects\\propreturns_assignment\\Mumbai_Real_Estate_Data\\pdfs\\1.pdf'\n",
    "pdf = pdfplumber.open(save_path)\n",
    "page = pdf.pages[0]\n",
    "\n",
    "table_settings = {\n",
    "    \"vertical_strategy\": \"explicit\",\n",
    "    \"horizontal_strategy\": \"explicit\",\n",
    "    \"explicit_vertical_lines\": curves_to_edges(page.curves + page.edges),\n",
    "    \"explicit_horizontal_lines\": curves_to_edges(page.curves + page.edges),\n",
    "    \"intersection_y_tolerance\": 10,\n",
    "}\n",
    "\n",
    "bboxes = [table.bbox for table in page.find_tables(table_settings=table_settings)]\n",
    "\n",
    "non_table_data = page.filter(not_within_bboxes).extract_text()\n",
    "\n",
    "table_data_in_nested_list = page.extract_table()\n",
    "cols = [\"sl_no\", \"pdf_urls\", \"save_path\", \"non_table_text\", \"table_dict\"]\n",
    "extracted_df = pd.DataFrame(columns = cols)\n",
    "table_data = {li[0]: li[1] for li in table_data_in_nested_list}\n",
    "curr_df = pd.DataFrame([[0, '', '', non_table_data, table_data]], columns = cols)\n",
    "extracted_df = pd.concat([extracted_df, curr_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df_2 = extracted_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_table(non_table_data, table_data):\n",
    "    visited = False\n",
    "    temp_dict = {}\n",
    "    temp_string = ''\n",
    "    for key, value in table_data.items():\n",
    "        combined_value = f'{key}__$__{value}' if visited else f'{key}__$__{value}__$__{non_table_data}'\n",
    "        combined_value = translator_v2(combined_value).split('__$__')\n",
    "        if not visited:\n",
    "            visited = True\n",
    "            temp_string = combined_value[2]\n",
    "        temp_dict[combined_value[0]] = combined_value[1]\n",
    "        \n",
    "    return pd.Series([temp_dict, temp_string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'(1)Subrogation type': 'Development agreement...</td>\n",
       "      <td>4286514 List  . 2 Second Director: Co-Dr. And...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  {'(1)Subrogation type': 'Development agreement...   \n",
       "\n",
       "                                                   1  \n",
       "0  4286514 List  . 2 Second Director: Co-Dr. And...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracted_df_2[[\"table_dict\", \"non_table_text\"]] = \n",
    "extracted_df_2[[\"table_dict\", \"non_table_text\"]].apply(lambda x: translate_table(x[\"non_table_text\"], x[\"table_dict\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df_2 = pd.concat([extracted_df_2, extracted_df_2[[\"table_dict\", \"non_table_text\"]].apply(lambda x: translate_table(x[\"non_table_text\"], x[\"table_dict\"]),axis=1)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df_2 = extracted_df_2.rename(columns = {0: \"table_data\", 1: \"non_table_data\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl_no</th>\n",
       "      <th>pdf_urls</th>\n",
       "      <th>save_path</th>\n",
       "      <th>non_table_text</th>\n",
       "      <th>table_dict</th>\n",
       "      <th>table_data</th>\n",
       "      <th>non_table_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4286514 सचू ी  . २ दुयम िनबंधक :सह दु.िन. अं...</td>\n",
       "      <td>{'(1)दतऐवज कार': 'िवकसनकरारनामा', '(2)मोबदला...</td>\n",
       "      <td>{'(1)Substitute type': 'Development agreement'...</td>\n",
       "      <td>4286514 list . 2 Second Director: Co-Dr. Andhe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sl_no pdf_urls save_path                                     non_table_text  \\\n",
       "0     0                     4286514 सचू ी  . २ दुयम िनबंधक :सह दु.िन. अं...   \n",
       "\n",
       "                                          table_dict  \\\n",
       "0  {'(1)दतऐवज कार': 'िवकसनकरारनामा', '(2)मोबदला...   \n",
       "\n",
       "                                          table_data  \\\n",
       "0  {'(1)Substitute type': 'Development agreement'...   \n",
       "\n",
       "                                      non_table_data  \n",
       "0  4286514 list . 2 Second Director: Co-Dr. Andhe...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
