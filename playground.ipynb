{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.selenium_manager import SeleniumManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "from googletrans import Translator\n",
    "from google_trans_new import google_translator  \n",
    "from deep_translator import GoogleTranslator\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from tqdm import tqdm\n",
    "import pyspark.pandas as ps\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def translator(s,source):\n",
    "    return GoogleTranslator(source=source, target='en').translate(s)\n",
    "\n",
    "\n",
    "def translator_v2(s,source='auto'):\n",
    "        translator = Translator()\n",
    "        return translator.translate(s, src='hi',dest='en').text if s != '' else 'null' \n",
    "\n",
    "translate_udf = udf(translator_v2,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbselect 2023\n",
      "Clicked 2023 for dbselect\n",
      "district_id मुंबई उपनगर\n",
      "Clicked मुंबई उपनगर for district_id\n",
      "taluka_id अंधेरी\n",
      "Clicked अंधेरी for taluka_id\n",
      "village_id बांद्रा\n",
      "Clicked बांद्रा for village_id\n",
      "Entered Captcha 18PUK2\n",
      "Set to All Records\n",
      "So Far, So Good\n"
     ]
    }
   ],
   "source": [
    "#selenium webdriver\n",
    "input_params_dropdown_by_id = {}\n",
    "input_params_dropdown_by_id['dbselect'] = '2023' #Select Year\n",
    "input_params_dropdown_by_id['district_id'] = 'मुंबई उपनगर' #District\n",
    "input_params_dropdown_by_id['taluka_id'] = 'अंधेरी' #Taluka\n",
    "input_params_dropdown_by_id['village_id'] = 'बांद्रा' #Village\n",
    "\n",
    "input_params_text_by_id = {}\n",
    "input_params_text_by_id['free_text'] = '2023' \n",
    "\n",
    "url = 'https://pay2igr.igrmaharashtra.gov.in/eDisplay/Propertydetails/index' \n",
    "driver = webdriver.Chrome() \n",
    "driver.implicitly_wait(10)\n",
    "driver.get(url)\n",
    "\n",
    "for key, value in input_params_dropdown_by_id.items():\n",
    "\tprint(key,value)\n",
    "\tdropdown = driver.find_element(by=By.ID,value=key)\n",
    "\tdropdown_select = Select(dropdown)\n",
    "\tfor option in dropdown_select.options:\n",
    "\t\tif option.text == value:\n",
    "\t\t\toption.click()\n",
    "\t\t\tprint(f'Clicked {option.text} for {key}')\n",
    "\t\t\tbreak\n",
    "\tdriver.implicitly_wait(2)\n",
    "\n",
    "#Input reg year\n",
    "driver.find_element(by=By.ID, value='free_text').send_keys(input_params_text_by_id['free_text'])\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "#Input Captcha\n",
    "captcha = input()\n",
    "driver.find_element(by=By.ID, value='cpatchaTextBox').send_keys(captcha)\n",
    "print(f'Entered Captcha {captcha}')\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "#Submit to get result set\n",
    "driver.find_element(by=By.ID,value='submit').click()\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "\n",
    "#Click on 50 Pages\n",
    "dropdown = driver.find_element(by=By.NAME, value='tableparty_length')\n",
    "dropdown_select = Select(dropdown)\n",
    "for option in dropdown_select.options:\n",
    "\tif option.text == 'All':\n",
    "\t\toption.click()\n",
    "\t\tprint('Set to All Records')\n",
    "\t\tbreak\n",
    "\n",
    "print('So Far, So Good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1209/1209 [03:09<00:00,  6.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#scrap records\n",
    "records_raw = pd.DataFrame(columns=['अनु क्र.','दस्त क्र.','दस्त प्रकार','दू. नि. कार्यालय','वर्ष','लिहून देणार','लिहून घेणार','इतर माहीती','सूची क्र. २'])\n",
    "\n",
    "print('Data Loading...')\n",
    "for index, table in enumerate(tqdm(driver.find_elements(by=By.ID, value='tbdata'))):\n",
    "    data = [item.text if item.text != 'सूची क्र. २' else item.find_element(by=By.TAG_NAME,value='a').get_attribute('href') for item in table.find_elements(by=By.XPATH, value=\".//*[self::td or self::th]\")]\n",
    "    records_raw.loc[len(records_raw)] = data\n",
    "    # print(data)\n",
    "    # print(f'{index} Row appended',)\n",
    "\n",
    "print('Finished!')\n",
    "records_raw\n",
    "records_translate = records_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209 DataFrame has been inserted into the 'record_details_raw' table.\n"
     ]
    }
   ],
   "source": [
    "# Insert Scraped Raw Data in postgres\n",
    "db_user = 'postgres'\n",
    "db_password = 'Sunrise12345'\n",
    "db_host = '127.0.0.1'\n",
    "db_port = '5432'\n",
    "db_name = 'propReturns'\n",
    "\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "table_name = 'record_details_raw'\n",
    "\n",
    "records_raw.to_sql(table_name, engine, if_exists='replace', index=True)\n",
    "\n",
    "print(f\"{len(records_raw)} DataFrame has been inserted into the '{table_name}' table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -> [0,100]\n",
      "Translating diarrhea type\n",
      "Done\n",
      "Translating Du. Prohibit. Office\n",
      "Done\n",
      "Translating Will write\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "Request exception can happen due to an api connection error. Please check your connection and try again",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_to_translate:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     temp_df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m records_translated \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([records_translated,temp_df])\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_to_translate:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     temp_df[col] \u001b[38;5;241m=\u001b[39m temp_df[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m records_translated \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([records_translated,temp_df])\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mtranslator\u001b[1;34m(s, source)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslator\u001b[39m(s,source):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGoogleTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\deep_translator\\google.py:74\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_failed(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code):\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError()\n\u001b[0;32m     76\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m element \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_tag, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_query)\n",
      "\u001b[1;31mRequestError\u001b[0m: Request exception can happen due to an api connection error. Please check your connection and try again"
     ]
    }
   ],
   "source": [
    "#Approach Via Pandas --> Extremely Slow\n",
    "records_translate = records_raw.copy()\n",
    "records_translate.columns = [translator(col,source='auto') for col in records_translate.columns]\n",
    "cols_to_translate = [\n",
    "    'diarrhea type',\n",
    "    'Du. Prohibit. Office',\n",
    "    'Will write', \n",
    "    'Will write down', \n",
    "    'Other information'\n",
    "    ]\n",
    "\n",
    "records_translated = pd.DataFrame(columns=records_translate.columns)\n",
    "batch_size = 100\n",
    "start = 0\n",
    "for batch in range(start,len(records_translate),batch_size):\n",
    "    print(f'Batch -> [{batch},{batch+batch_size}]')\n",
    "    temp_df = records_translate.iloc[batch:batch+batch_size,:].copy()\n",
    "    for col in cols_to_translate:\n",
    "        print(f'Translating {col}')\n",
    "        temp_df[col] = temp_df[col].apply(lambda x: translator(x,source='hi'))\n",
    "        print(f'Done')\n",
    "    records_translated = pd.concat([records_translated,temp_df])\n",
    "records_translated.rename(columns={'Will write':'buyer_name','Will write down':'seller_name'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del os.environ['PYSPARK_PYTHON'] \n",
    "# del os.environ['PYSPARK_DRIVER_PYTHON'] \n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable \n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "del spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TQDMSparkExample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29c25177790>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach Via Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# spark = SparkSession.builder \\\n",
    "#         .config('spark.executor.instances', 4) \\\n",
    "#         .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "#         .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "#         .config(\"spark.dynamicAllocation.minExecutors\",\"1\") \\\n",
    "#         .config(\"spark.dynamicAllocation.maxExecutors\",\"5\") \\\n",
    "#         .config(\"spark.executor.cores\",6) \\\n",
    "#         .config(\"spark.executor.memory\",\"2g\") \\\n",
    "#         .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "# #spark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.executor.cores\", 4)\n",
    "# spark.conf.set(\"spark.dynamicAllocation.minExecutors\",\"1\")\n",
    "# spark.conf.set(\"spark.dynamicAllocation.maxExecutors\",\"5\")\n",
    "# conf = spark.sparkContext.getConf()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Data from SQL directly\n",
    "host = '127.0.0.1'\n",
    "port = '5432'\n",
    "database = 'propReturns'\n",
    "username = 'postgres'\n",
    "password = 'Sunrise12345'\n",
    "url = f\"jdbc:postgresql://{host}:{port}/{database}\"\n",
    "table_name = 'record_details_raw'\n",
    "\n",
    "engine = create_engine(f'postgresql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "spark_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password).load()\n",
    "\n",
    "records_raw = pd.read_sql('record_details_raw',engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for col in records_raw.columns:\n",
    "    new_names.append(translator(col,source='auto'))\n",
    "\n",
    "new_names = [x.replace('.','') for x in new_names]\n",
    "spark_df = spark_df.toDF(*new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|index|Sl no|Diarrhea no|       diarrhea type|Du Prohibit Office|      Year|          Will write|     Will write down|   Other information|           List no 2|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|    1|      13217|           भाडेपट्टा|सह दु.नि. अंधेरी 7|25/07/2023|1) श्रीमती चंद्रक...|1) महाराष्ट़ हाऊस...|1) इतर माहिती: पि...|https://pay2igr.i...|\n",
      "|    1|    2|       4286|       विकसनकरारनामा|सह दु.नि. अंधेरी 7|10/03/2023|1) अजित सिंह करता...|1) नाविश रियल्टी ...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|    2|    3|       2449|             सेल डीड|सह दु.नि. अंधेरी 7|09/02/2023|1) नूपुर अनिल कळक...|1) हर्ष सतपाल मल्...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|    3|    4|       8691|66-नोटीस ऑफ़ लिस प...|सह दु.नि. अंधेरी 7|22/05/2023|      1) प्रदीप सोनी|                    |1) इतर माहिती: सि...|https://pay2igr.i...|\n",
      "|    4|    5|       3551|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|27/02/2023|1) गुंजन योगीत कप...|1) शामा सुभाष ओबेराय|1) सदनिका नं: 201...|https://pay2igr.i...|\n",
      "|    5|    6|       8043|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|19/05/2023|1) राधिका खंडेलवा...|1) व्हीबीएचडीसी ब...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   12|   13|       7415|       विकसनकरारनामा|सह दु.नि. अंधेरी 7|02/05/2023|1) सोसायटी मेंबर ...|1) इंस्पायरा बिल्...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|    6|    7|       5454|           भाडेपट्टा|सह दु.नि. अंधेरी 7|29/03/2023|1) नियोजन को ऑपरे...|1) मुंबई हाऊसिंग ...|1) सदनिका नं: प्ल...|https://pay2igr.i...|\n",
      "|    7|    8|       7732|    अभिहस्तांतरणपत्र|सह दु.नि. अंधेरी 7|04/05/2023|1) मे एकता सुप्री...|1) युडोरा कॉ ऑप ह...|1) इतर माहिती: इम...|https://pay2igr.i...|\n",
      "|    8|    9|       5457|           भाडेपट्टा|सह दु.नि. अंधेरी 7|29/03/2023|1) साई दत्त प्रसा...|1) मुंबई हाऊसिंग ...|1) सदनिका नं: प्ल...|https://pay2igr.i...|\n",
      "|    9|   10|       8896|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|24/05/2023|1) . . गोपाल नारा...|1) . . श्रीधर गोप...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|   10|   11|       2389|       प्रतिज्ञापत्र|सह दु.नि. अंधेरी 7|08/02/2023|1) गांघी नगर गणेश...|                    |1) इतर माहिती: re...|https://pay2igr.i...|\n",
      "|   11|   12|       8895|65-चुक दुरुस्ती पत्र|सह दु.नि. अंधेरी 7|24/05/2023|1) . . गोपाल नारा...|1) . . नेहा गोपाल...|1) इतर माहिती: सद...|https://pay2igr.i...|\n",
      "|   13|   14|        212|             सेल डीड|सह दु.नि. अंधेरी 7|04/01/2023|1) पुष्पा बी रहेज...|1) कलश दिलीप सुरा...|1) सदनिका नं: 501...|https://pay2igr.i...|\n",
      "|   14|   15|       7019|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|21/04/2023|1) मिशेल देसाई पू...|1) किऑर्बिट रिअल्...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   15|   16|       5868|डेव्हलपमेंट अँग्र...|सह दु.नि. अंधेरी 7|12/04/2023|1) बांद्रा विनय क...|1) डायनास्टी इन्फ...|1) इतर माहिती: जम...|https://pay2igr.i...|\n",
      "|   16|   17|       5450|              लीजडीड|सह दु.नि. अंधेरी 7|29/03/2023|1) दि एम आय जी को...|1) (मालक ) महाराष...|1) इतर माहिती: टी...|https://pay2igr.i...|\n",
      "|   17|   18|       3753|             सेल डीड|सह दु.नि. अंधेरी 7|01/03/2023|1) राहुल विनायक ड...|1) रमेशकुमार मानक...|1) सदनिका नं: 801...|https://pay2igr.i...|\n",
      "|   18|   19|       8517|66-नोटीस ऑफ़ लिस प...|सह दु.नि. अंधेरी 7|18/05/2023|1) व्हिज एंटरप्रा...|                    |1) सदनिका नं: 602...|https://pay2igr.i...|\n",
      "|   49|   50|        496| लिव्ह अॅड लायसन्सेस|सह दु.नि. अंधेरी 7|10/01/2023|1) लीना किरण निसा...|1) रैलोगिक कंट्रो...|1) फ़्लॅट नं:एल जी...|https://pay2igr.i...|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_translate = [\n",
    "    'diarrhea type',\n",
    "    'Du Prohibit Office',\n",
    "    'Will write', \n",
    "    'Will write down', \n",
    "    'Other information'\n",
    "    ]\n",
    "\n",
    "for column in cols_to_translate:\n",
    "    spark_df = spark_df.withColumn(column, translate_udf(spark_df[column]))\n",
    "\n",
    "spark_df = spark_df.withColumnRenamed(existing='Will write',new='buyer_name')\n",
    "spark_df = spark_df.withColumnRenamed(existing='Will write down',new='seller_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|index|Sl no|Diarrhea no|       diarrhea type|Du Prohibit Office|      Year|          buyer_name|         seller_name|   Other information|           List no 2|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|    1|       4286|development agree...|Co. D.N. Andheri 7|10/03/2023|1) Ajit Singh Kar...|1) Paresh Ranchho...|1) Other informat...|https://pay2igr.i...|\n",
      "|    1|    2|      13217|               lease|Co. D.N. Andheri 7|25/07/2023|1) Bhavin Sheth, ...|1) Hanmant Dhanva...|1) Other informat...|https://pay2igr.i...|\n",
      "|    2|    3|       2449|           sale deed|Co. D.N. Andheri 7|09/02/2023|1) Nupur Anil Kal...|1) Harsh Satpal M...|1) Other Informat...|https://pay2igr.i...|\n",
      "|    3|    4|       8691|66-Notice of list...|Co. D.N. Andheri 7|22/05/2023|     1) Pradeep Soni|                null|1) Other Informat...|https://pay2igr.i...|\n",
      "|    4|    5|       3551|65-correction letter|Co. D.N. Andheri 7|27/02/2023|1) Gunjan Yogeet ...|1) Shama Subhash ...|1) House No: 201,...|https://pay2igr.i...|\n",
      "|    5|    6|       8043|development agree...|Co. D.N. Andheri 7|19/05/2023|1) Radhika Khande...|1) Suraj Kumar Du...|1) Other informat...|https://pay2igr.i...|\n",
      "|   12|   13|       7415|development agree...|Co. D.N. Andheri 7|02/05/2023|1) सोसायटी मेंबर ...|1) Inspira Buildc...|1) Other informat...|https://pay2igr.i...|\n",
      "|    6|    7|       5454|               lease|Co. D.N. Andheri 7|29/03/2023|1) Planning was t...|1) T R Chatupule,...|1) House No.: Plo...|https://pay2igr.i...|\n",
      "|    7|    8|       5457|               lease|Co. D.N. Andheri 7|29/03/2023|1) Sai Dutt Prasa...|1) Ashok Kajne, E...|1) House No.: Plo...|https://pay2igr.i...|\n",
      "|    8|    9|       7732|    deed of transfer|Co. D.N. Andheri 7|04/05/2023|1) Chief Executiv...|1) Khazindar Rama...|1) Other Details:...|https://pay2igr.i...|\n",
      "|    9|   10|       8895|65-correction letter|Co. D.N. Andheri 7|24/05/2023|1) . , Gopal Nara...|1) . , Neha Gopal...|1) Other informat...|https://pay2igr.i...|\n",
      "|   10|   11|       8896|65-correction letter|Co. D.N. Andheri 7|24/05/2023|1) . , Gopal Nara...|1) . , Sridhar Go...|1) Other informat...|https://pay2igr.i...|\n",
      "|   11|   12|       2389|     promissory note|Co. D.N. Andheri 7|08/02/2023|1) Gandhi Nagar G...|                null|1) Other informat...|https://pay2igr.i...|\n",
      "|   13|   14|        212|           sale deed|Co. D.N. Andheri 7|04/01/2023|1) Mukhtyar Rishi...|1) Kalash Dilip S...|1) House No: 501,...|https://pay2igr.i...|\n",
      "|   14|   15|       7019|development agree...|Co. D.N. Andheri 7|21/04/2023|1) Michel Desai P...|1) Chief Geeta Mo...|1) Other informat...|https://pay2igr.i...|\n",
      "|   15|   16|       5868|development agree...|Co. D.N. Andheri 7|12/04/2023|1) Chairman Jaywa...|1) Krunal Sheth, ...|1) Other informat...|https://pay2igr.i...|\n",
      "|   16|   17|       5450|           leasedeed|Co. D.N. Andheri 7|29/03/2023|1) Vijay Fatarfek...|1) (Owner) Neelim...|1) Other Informat...|https://pay2igr.i...|\n",
      "|   17|   18|       3753|           sale deed|Co. D.N. Andheri 7|01/03/2023|1) Rahul Vinayak ...|1) Rameshkumar Ma...|1) House No: 801 ...|https://pay2igr.i...|\n",
      "|   18|   19|       8517|66-Notice of list...|Co. D.N. Andheri 7|18/05/2023|1) Whiz Enterpris...|                null|1) House No: 602,...|https://pay2igr.i...|\n",
      "|   33|   34|       3172|    live ad licenses|Co. D.N. Andheri 7|20/02/2023|      1) Bimla Devi.|1) Puriben Bachch...|1) Flat No: Plot ...|https://pay2igr.i...|\n",
      "+-----+-----+-----------+--------------------+------------------+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o144.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:570)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:82)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:570)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m table_name_sink \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord_details_translated\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name_sink\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o144.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (host.docker.internal executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:570)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:82)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:179)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:570)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "table_name_sink = 'record_details_translated'\n",
    "\n",
    "spark_df.select(\"*\").write.format(\"jdbc\")\\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", table_name_sink) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_translated = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Sl no</th>\n",
       "      <th>Diarrhea no</th>\n",
       "      <th>diarrhea type</th>\n",
       "      <th>Du Prohibit Office</th>\n",
       "      <th>Year</th>\n",
       "      <th>buyer_name</th>\n",
       "      <th>seller_name</th>\n",
       "      <th>Other information</th>\n",
       "      <th>List no 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13217</td>\n",
       "      <td>lease</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>25/07/2023</td>\n",
       "      <td>1) Bhavin Sheth, Authorized Trustee on behalf ...</td>\n",
       "      <td>1) Hanmant Dhanvare, Executive Engineer, Mahar...</td>\n",
       "      <td>1) Other information: Piece or parcel of land ...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4286</td>\n",
       "      <td>development agreement</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>10/03/2023</td>\n",
       "      <td>1) Ajit Singh Kartar Singh Chandok</td>\n",
       "      <td>1) Paresh Ranchhod Patel, Partner, Navish Real...</td>\n",
       "      <td>1) Other information: Land and construction, p...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2449</td>\n",
       "      <td>sale deed</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>09/02/2023</td>\n",
       "      <td>1) Nupur Anil Kalke by Mukhtyar Shaila Anil Ka...</td>\n",
       "      <td>1) Harsh Satpal Malhotra</td>\n",
       "      <td>1) Other Information: House No: 701, Floor No:...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8691</td>\n",
       "      <td>66-Notice of list pendency</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>22/05/2023</td>\n",
       "      <td>1) Pradeep Soni</td>\n",
       "      <td>null</td>\n",
       "      <td>1) Other Information: City Civil Court at Dind...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3551</td>\n",
       "      <td>65-correction letter</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>27/02/2023</td>\n",
       "      <td>1) Gunjan Yogeet Kapoor\\n2) Tushar Subhash Oberoi</td>\n",
       "      <td>1) Shama Subhash Oberoi</td>\n",
       "      <td>1) House No: 201, Mala No: 2, Building Boat: O...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>1204</td>\n",
       "      <td>1205</td>\n",
       "      <td>3173</td>\n",
       "      <td>live ad licenses</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>20/02/2023</td>\n",
       "      <td>1) Coelho Noala</td>\n",
       "      <td>1) Muhammad Kunhi KC</td>\n",
       "      <td>1) Flat No:35/A, Floor No:1ST FLOOR, Building ...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>1205</td>\n",
       "      <td>1206</td>\n",
       "      <td>2531</td>\n",
       "      <td>agreement</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>13/02/2023</td>\n",
       "      <td>1) Chief Sandeep Gawde on behalf of Chandresh ...</td>\n",
       "      <td>1) Rashmi Satish Kewalramani\\n2) Varun Satish ...</td>\n",
       "      <td>1) Other Information: Real Estate Project-\\\"Ru...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>1206</td>\n",
       "      <td>1207</td>\n",
       "      <td>2863</td>\n",
       "      <td>live ad licenses</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>15/02/2023</td>\n",
       "      <td>1) Faisal Abdul Rahim Ghori</td>\n",
       "      <td>1) Jeram Chamadia (HUF) Laxman</td>\n",
       "      <td>1) Flat No.: Garage No. 1, Floor No.: Ground, ...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>1207</td>\n",
       "      <td>1208</td>\n",
       "      <td>2506</td>\n",
       "      <td>release deed</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>10/02/2023</td>\n",
       "      <td>1) Monisha Rahim Harji alias Monisha B.Charani...</td>\n",
       "      <td>1) Karim B. pasture</td>\n",
       "      <td>1) House No: 1201, Floor No: Barawa Majla, Bui...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>1208</td>\n",
       "      <td>1209</td>\n",
       "      <td>6172</td>\n",
       "      <td>deed of transfer</td>\n",
       "      <td>Co. D.N. Andheri 7</td>\n",
       "      <td>11/04/2023</td>\n",
       "      <td>1) Mr. Ashish Nalwaya, Partner, Saffron Ambit ...</td>\n",
       "      <td>1) Mr. Pratyush Bharatiya, Partner, Saffron In...</td>\n",
       "      <td>1) Other information: Total area of ​​land and...</td>\n",
       "      <td>https://pay2igr.igrmaharashtra.gov.in/eDisplay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1209 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index Sl no Diarrhea no               diarrhea type  Du Prohibit Office  \\\n",
       "0         0     1       13217                       lease  Co. D.N. Andheri 7   \n",
       "1         1     2        4286       development agreement  Co. D.N. Andheri 7   \n",
       "2         2     3        2449                   sale deed  Co. D.N. Andheri 7   \n",
       "3         3     4        8691  66-Notice of list pendency  Co. D.N. Andheri 7   \n",
       "4         4     5        3551        65-correction letter  Co. D.N. Andheri 7   \n",
       "...     ...   ...         ...                         ...                 ...   \n",
       "1204   1204  1205        3173            live ad licenses  Co. D.N. Andheri 7   \n",
       "1205   1205  1206        2531                   agreement  Co. D.N. Andheri 7   \n",
       "1206   1206  1207        2863            live ad licenses  Co. D.N. Andheri 7   \n",
       "1207   1207  1208        2506                release deed  Co. D.N. Andheri 7   \n",
       "1208   1208  1209        6172            deed of transfer  Co. D.N. Andheri 7   \n",
       "\n",
       "            Year                                         buyer_name  \\\n",
       "0     25/07/2023  1) Bhavin Sheth, Authorized Trustee on behalf ...   \n",
       "1     10/03/2023                 1) Ajit Singh Kartar Singh Chandok   \n",
       "2     09/02/2023  1) Nupur Anil Kalke by Mukhtyar Shaila Anil Ka...   \n",
       "3     22/05/2023                                    1) Pradeep Soni   \n",
       "4     27/02/2023  1) Gunjan Yogeet Kapoor\\n2) Tushar Subhash Oberoi   \n",
       "...          ...                                                ...   \n",
       "1204  20/02/2023                                    1) Coelho Noala   \n",
       "1205  13/02/2023  1) Chief Sandeep Gawde on behalf of Chandresh ...   \n",
       "1206  15/02/2023                        1) Faisal Abdul Rahim Ghori   \n",
       "1207  10/02/2023  1) Monisha Rahim Harji alias Monisha B.Charani...   \n",
       "1208  11/04/2023  1) Mr. Ashish Nalwaya, Partner, Saffron Ambit ...   \n",
       "\n",
       "                                            seller_name  \\\n",
       "0     1) Hanmant Dhanvare, Executive Engineer, Mahar...   \n",
       "1     1) Paresh Ranchhod Patel, Partner, Navish Real...   \n",
       "2                              1) Harsh Satpal Malhotra   \n",
       "3                                                  null   \n",
       "4                               1) Shama Subhash Oberoi   \n",
       "...                                                 ...   \n",
       "1204                               1) Muhammad Kunhi KC   \n",
       "1205  1) Rashmi Satish Kewalramani\\n2) Varun Satish ...   \n",
       "1206                     1) Jeram Chamadia (HUF) Laxman   \n",
       "1207                                1) Karim B. pasture   \n",
       "1208  1) Mr. Pratyush Bharatiya, Partner, Saffron In...   \n",
       "\n",
       "                                      Other information  \\\n",
       "0     1) Other information: Piece or parcel of land ...   \n",
       "1     1) Other information: Land and construction, p...   \n",
       "2     1) Other Information: House No: 701, Floor No:...   \n",
       "3     1) Other Information: City Civil Court at Dind...   \n",
       "4     1) House No: 201, Mala No: 2, Building Boat: O...   \n",
       "...                                                 ...   \n",
       "1204  1) Flat No:35/A, Floor No:1ST FLOOR, Building ...   \n",
       "1205  1) Other Information: Real Estate Project-\\\"Ru...   \n",
       "1206  1) Flat No.: Garage No. 1, Floor No.: Ground, ...   \n",
       "1207  1) House No: 1201, Floor No: Barawa Majla, Bui...   \n",
       "1208  1) Other information: Total area of ​​land and...   \n",
       "\n",
       "                                              List no 2  \n",
       "0     https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "1     https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "2     https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "3     https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "4     https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "...                                                 ...  \n",
       "1204  https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "1205  https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "1206  https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "1207  https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "1208  https://pay2igr.igrmaharashtra.gov.in/eDisplay...  \n",
       "\n",
       "[1209 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# records_translated = spark_df.toPandas()\n",
    "table_name = 'record_details_translated'\n",
    "records_translated.to_sql(table_name, engine, if_exists='replace', index=True)\n",
    "print(f\"{len(records_translate)} DataFrame has been inserted into the '{table_name}' table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
